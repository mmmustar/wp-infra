This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-04T13:53:36.707Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
ansible/
  group_vars/
    all.yml
  inventory/
    hosts.yml
  roles/
    common/
      defaults/
        main.yml
      files/
        get_secret.py
      handlers/
        main.yml
      tasks/
        main.yml
    docker/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        main.yml
    k3s/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        ebs-csi.yml
        helm.yml
        main.yml
      templates/
        ebs-csi-values.yml.j2
    nginx/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        main.yml
      templates/
        wordpress.conf.j2
    wordpress/
      defaults/
        main.yml
      tasks/
        ingress.yml
        main.yml
        secrets.yml
      templates/
        configmap.yaml.j2
        deployment.yaml.j2
        ingress.yaml.j2
        secret.yaml.j2
        service.yaml.j2
        storage.yaml.j2
  ansible.cfg
  requirements.yml
  run.sh
  site.yml
environments/
  backend-config/
    .terraform.lock.hcl
    main.tf
    outputs.tf
    variables.tf
    versions.tf
  modules/
    compute/
      main.tf
      outputs.tf
      variables.tf
    k3s/
      iam.tf
    network/
      main.tf
      outputs.tf
      security.tf
      variables.tf
      vpc-peering.tf
    security/
      main.tf
      outputs.tf
      variables.tf
  prod/
    backend.tf
  test/
    .terraform.lock.hcl
    backend.tf
    main.tf
    variables.tf
scripts/
  ssh-update.sh
-n ingress-nginx
.gitignore
repomix-output-1.txt

================================================================
Files
================================================================

================
File: ansible/group_vars/all.yml
================
# ansible/group_vars/all.yml
---
# Variables globales communes à tous les serveurs
timezone: UTC
environment: test
aws_region: eu-west-3
ebs_csi_enabled: true
cloud_provider: aws
k8s_storage_class: ebs-sc

================
File: ansible/inventory/hosts.yml
================
all:
  vars:
    ansible_python_interpreter: /usr/bin/python3
    pip_package: python3-pip
    pip_install_packages:
      - name: boto3
  children:
    wordpress:
      hosts:
        wp-test:
          ansible_host: 35.180.222.29
          ansible_user: ubuntu
          ansible_ssh_private_key_file: ~/.ssh/test-aws-key-pair-new.pem

================
File: ansible/roles/common/defaults/main.yml
================
# roles/common/defaults/main.yml
---
system_packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - software-properties-common
  - python3-pip
  - python3-boto3
  - python3-botocore
  - nfs-common
  - git
  - vim
timezone: UTC
environment: test

================
File: ansible/roles/common/files/get_secret.py
================
#!/usr/bin/env python3
import boto3
import json
import sys

try:
    client = boto3.client('secretsmanager', region_name='eu-west-3')
    response = client.get_secret_value(SecretId='book')
    print(response['SecretString'])
except Exception as e:
    print(f"Erreur: {str(e)}", file=sys.stderr)
    sys.exit(1)

================
File: ansible/roles/common/handlers/main.yml
================
# roles/common/handlers/main.yml
---
- name: restart systemd-resolved
  service:
    name: systemd-resolved
    state: restarted

================
File: ansible/roles/common/tasks/main.yml
================
---
- name: Wait for cloud-init to complete  
  command: cloud-init status --wait  
  register: cloud_init_result  
  until: cloud_init_result.rc == 0  
  retries: 30  
  delay: 10  
  changed_when: false
  
- name: Reset connection to allow group changes to take effect  
  meta: reset_connection

- name: Wait for apt lock  
  shell: while sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1 || sudo fuser /var/lib/apt/lists/lock >/dev/null 2>&1; do sleep 1; done  
  changed_when: false
  
- name: Clean apt  
  apt:
    clean: yes  
    autoclean: yes  
    autoremove: yes  
    force_apt_get: yes  
  become: true

- name: Force kill apt/dpkg processes  
  shell: |
    killall apt apt-get dpkg 2>/dev/null || true  
    rm -f /var/lib/apt/lists/lock /var/cache/apt/archives/lock /var/lib/dpkg/lock* 2>/dev/null || true  
    dpkg --configure -a  
  changed_when: false  
  ignore_errors: true  
  become: true

- name: Update package cache  
  apt:
    update_cache: yes  
    cache_valid_time: 3600  
    force_apt_get: yes  
  register: apt_update  
  retries: 5  
  delay: 10  
  until: apt_update is success  
  become: true

- name: Install system packages  
  apt:
    name:
      - python3  
      - python3-pip  
      - python3-boto3  
      - python3-botocore  
      - python3-kubernetes  
      - python3-openshift  
      - git  
      - vim  
    state: present  
    force_apt_get: yes  
    update_cache: yes  
  become: true  
  register: pkg_install  
  retries: 3  
  delay: 10  
  until: pkg_install is success

================
File: ansible/roles/docker/defaults/main.yml
================
# roles/docker/defaults/main.yml
---
docker_users:
  - "{{ ansible_user }}"

================
File: ansible/roles/docker/handlers/main.yml
================
# roles/docker/handlers/main.yml
---
- name: restart docker
  service:
    name: docker
    state: restarted

================
File: ansible/roles/docker/tasks/main.yml
================
- name: Install required packages
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg-agent
      - software-properties-common
    state: present

- name: Add Docker GPG key
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: Add Docker repository
  apt_repository:
    repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable
    state: present

- name: Install Docker
  apt:
    name:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - docker-compose-plugin
    state: present
    update_cache: yes

- name: Start and enable Docker
  service:
    name: docker
    state: started
    enabled: yes

- name: Add users to docker group
  user:
    name: "{{ item }}"
    groups: docker
    append: yes
  with_items: "{{ docker_users }}"

- name: Configure Docker daemon
  copy:
    content: |
      {
        "log-driver": "json-file",
        "log-opts": {
          "max-size": "100m",
          "max-file": "3"
        }
      }
    dest: /etc/docker/daemon.json
  notify: restart docker

================
File: ansible/roles/k3s/defaults/main.yml
================
# roles/k3s/defaults/main.yml
---
k3s_version: "v1.28.4+k3s2"
k3s_server_args: "--disable traefik --disable servicelb"
ebs_csi_enabled: true
ebs_csi_version: "2.26.0"
storage_class_name: "ebs-sc"
aws_region: "eu-west-3"
wp_namespace: wordpress
kubeconfig: /etc/rancher/k3s/k3s.yaml
security_group_id: "sg-0e2ab25ac27f8bbc5"
project_name: "wordpress-mmustar"
environment: "test"

================
File: ansible/roles/k3s/handlers/main.yml
================
# roles/k3s/handlers/main.yml
---
- name: restart k3s
  service:
    name: k3s
    state: restarted

- name: wait for k3s
  wait_for:
    path: "{{ kubeconfig }}"
    delay: 10
    timeout: 300

================
File: ansible/roles/k3s/tasks/ebs-csi.yml
================
# roles/k3s/tasks/ebs-csi.yml
---
- name: Fetch AWS Secrets
  command: "aws secretsmanager get-secret-value --secret-id book --region {{ aws_region }} --output json"
  register: secrets_result
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set variables from secrets
  set_fact:
    aws_account_id: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_account_id') }}"
    aws_ebs_csi_role_name: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_ebs_csi_role_name') }}"

- name: Check if EBS CSI Driver is already installed
  command: helm list -n kube-system
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_list
  changed_when: false

- name: Delete old StorageClass if exists
  command: kubectl delete storageclass ebs-sc
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  ignore_errors: true
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Add EBS CSI Helm repository
  kubernetes.core.helm_repository:
    name: aws-ebs-csi-driver
    repo_url: https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    state: present
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Update Helm repositories
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

- name: Template EBS CSI values
  template:
    src: ebs-csi-values.yml.j2
    dest: /tmp/ebs-csi-values.yml
    mode: '0644'
  when: ebs_csi_enabled

- name: Create kube-system namespace if not exists
  kubernetes.core.k8s:
    name: kube-system
    api_version: v1
    kind: Namespace
    state: present
    kubeconfig: /etc/rancher/k3s/k3s.yaml

- name: Install AWS EBS CSI Driver
  kubernetes.core.helm:
    name: aws-ebs-csi-driver
    chart_ref: aws-ebs-csi-driver/aws-ebs-csi-driver
    release_namespace: kube-system
    create_namespace: true
    values_files:
      - /tmp/ebs-csi-values.yml
    chart_version: "{{ ebs_csi_version }}"
    wait: true
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Wait for EBS CSI Driver pods
  shell: >
    kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get pods -n kube-system 
    -l app.kubernetes.io/name=aws-ebs-csi-driver --no-headers
  register: ebs_csi_pods
  until: ebs_csi_pods.rc == 0 and ebs_csi_pods.stdout_lines | length > 0 and ebs_csi_pods.stdout.find('Running') != -1
  retries: 30
  delay: 10
  when: ebs_csi_enabled

- name: Clean up temporary values file
  file:
    path: /tmp/ebs-csi-values.yml
    state: absent
  when: ebs_csi_enabled

================
File: ansible/roles/k3s/tasks/helm.yml
================
# roles/k3s/tasks/helm.yml
---
- name: Add Helm apt key
  apt_key:
    url: https://baltocdn.com/helm/signing.asc
    state: present
  become: true

- name: Add Helm repository
  apt_repository:
    repo: deb https://baltocdn.com/helm/stable/debian/ all main
    state: present
    filename: helm
  become: true

- name: Install Helm
  apt:
    name: helm
    state: present
    update_cache: yes
  become: true
  register: helm_install
  retries: 3
  delay: 5
  until: helm_install is success

- name: Verify Helm installation
  command: helm version
  register: helm_version
  changed_when: false
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Add stable Helm repository
  command: helm repo add stable https://charts.helm.sh/stable
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true
  failed_when: false

- name: Update Helm repos
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

================
File: ansible/roles/k3s/tasks/main.yml
================
# roles/k3s/tasks/main.yml
---
# Étape 1: Configuration initiale
- name: Get subnet IDs from AWS
  shell: >
    aws ec2 describe-subnets
    --region {{ aws_region }}
    --filters "Name=tag:Project,Values={{ project_name }}" "Name=tag:Environment,Values={{ environment }}"
    --query "Subnets[0].SubnetId"
    --output text
  register: subnet_info
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set subnet ID
  set_fact:
    public_subnet_id: "{{ subnet_info.stdout }}"
  when: subnet_info.stdout != ""

# Étape 2: Installation de K3s
- name: Download k3s installation script
  get_url:
    url: https://get.k3s.io
    dest: /tmp/k3s-install.sh
    mode: '0755'

- name: Install k3s
  shell: 
    cmd: INSTALL_K3S_VERSION={{ k3s_version }} /tmp/k3s-install.sh --write-kubeconfig-mode 644 {{ k3s_server_args }}
  args:
    creates: /usr/local/bin/k3s
  environment:
    INSTALL_K3S_EXEC: "{{ k3s_server_args }}"
  register: k3s_install

- name: Wait for k3s service to start
  systemd:
    name: k3s
    state: started
    enabled: yes
  register: k3s_service

- name: Configure kubeconfig permissions
  file:
    path: /etc/rancher/k3s/k3s.yaml
    mode: '0644'
  become: true

- name: Set KUBECONFIG environment variable
  lineinfile:
    path: /home/{{ ansible_user }}/.bashrc
    line: "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml"
    state: present
  become: true

# Étape 3: Vérification de l'installation
- name: Wait for k3s node to be ready
  shell: kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes
  register: node_status
  until: "'Ready' in node_status.stdout"
  retries: 30
  delay: 10
  become: true

# Étape 4: Configuration du Security Group
- name: Add NodePort range to security group
  shell: >
    aws ec2 authorize-security-group-ingress 
    --region {{ aws_region }}
    --group-id {{ security_group_id }}
    --protocol tcp 
    --port 30000-32767 
    --cidr 0.0.0.0/0
  delegate_to: localhost
  become: false
  register: sg_result
  failed_when: 
    - sg_result.rc != 0 
    - '"InvalidPermission.Duplicate" not in sg_result.stderr'
  changed_when: sg_result.rc == 0

# Étape 5: Installation de Helm
- name: Include Helm installation tasks
  include_tasks: helm.yml

# Étape 6: Installation de NGINX Ingress
- name: Add ingress-nginx repository
  command: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true
  ignore_errors: true

- name: Update Helm repos
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

- name: Install ingress-nginx with Helm
  kubernetes.core.helm:
    name: ingress-nginx
    chart_ref: ingress-nginx/ingress-nginx
    release_namespace: ingress-nginx
    create_namespace: true
    release_values:
      controller:
        service:
          type: NodePort
          nodePorts:
            http: 32569
            https: 32570
        kind: DaemonSet
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  register: nginx_helm_result

- name: Wait for ingress-nginx pods to be Running
  shell: >
    kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller --no-headers
  register: nginx_pod_status
  until: nginx_pod_status.rc == 0 and 'Running' in nginx_pod_status.stdout
  retries: 30
  delay: 10

- name: Display ingress-nginx pod logs if not running
  shell: kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=50
  register: nginx_logs
  when: "'Running' not in nginx_pod_status.stdout"

- name: Show pod logs
  debug:
    var: nginx_logs.stdout_lines
  when: nginx_logs is defined

# Étape 7: Installation du EBS CSI Driver
- name: Include EBS CSI tasks
  include_tasks: ebs-csi.yml
  when: ebs_csi_enabled

# Étape 8: Vérification finale
- name: Verify cluster status
  command: kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes,pods --all-namespaces
  register: cluster_status
  changed_when: false

- name: Show cluster status
  debug:
    var: cluster_status.stdout_lines

================
File: ansible/roles/k3s/templates/ebs-csi-values.yml.j2
================
# roles/k3s/templates/ebs-csi-values.yml.j2
controller:
  region: "{{ aws_region }}"
  serviceAccount:
    create: true
    name: ebs-csi-controller-sa
    annotations:
      eks.amazonaws.com/role-arn: null
      
node:
  serviceAccount:
    create: true
    name: ebs-csi-node-sa

storageClasses:
- name: ebs-sc
  provisioner: ebs.csi.aws.com
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  volumeBindingMode: WaitForFirstConsumer
  allowVolumeExpansion: true
  reclaimPolicy: Delete
  parameters:
    type: gp3

================
File: ansible/roles/nginx/defaults/main.yml
================
domain_name: "test.mmustar.fr"
cert_email: "mmustar@free.fr"
ingress_http_nodeport: 32569

================
File: ansible/roles/nginx/handlers/main.yml
================
---
- name: Reload Nginx
  service:
    name: nginx
    state: reloaded
  become: true

- name: Restart Nginx
  service:
    name: nginx
    state: restarted
  become: true

- name: Enable Nginx
  service:
    name: nginx
    enabled: true
  become: true

- name: Test Nginx configuration
  command: nginx -t
  become: true
  register: nginx_config_test
  changed_when: false

================
File: ansible/roles/nginx/tasks/main.yml
================
---
# Installation de Nginx
- name: Install Nginx
  apt:
    name: nginx
    state: present
    update_cache: yes
  become: true

# Récupérer le NodePort de l'ingress
- name: Get ingress NodePort
  kubernetes.core.k8s_info:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    api_version: v1
    kind: Service
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: ingress_svc

- name: Set ingress port fact
  set_fact:
    ingress_http_nodeport: "{{ ingress_svc.resources[0].spec.ports | selectattr('name', 'equalto', 'http') | map(attribute='nodePort') | first }}"

# Configuration Nginx pour WordPress
- name: Create wordpress.conf
  template:
    src: wordpress.conf.j2
    dest: /etc/nginx/sites-available/wordpress.conf
    mode: '0644'
  become: true
  notify: Reload Nginx

- name: Enable wordpress site
  file:
    src: /etc/nginx/sites-available/wordpress.conf
    dest: /etc/nginx/sites-enabled/wordpress.conf
    state: link
  become: true
  notify: Reload Nginx

- name: Remove default nginx site
  file:
    path: /etc/nginx/sites-enabled/default
    state: absent
  become: true
  notify: Reload Nginx

- name: Ensure Nginx is started and enabled
  service:
    name: nginx
    state: started
    enabled: yes
  become: true

================
File: ansible/roles/nginx/templates/wordpress.conf.j2
================
server {
    listen 80;
    server_name {{ domain_name }};
    
    location / {
        proxy_pass http://127.0.0.1:{{ ingress_http_nodeport }};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Increased buffer settings
        proxy_buffers 16 16k;  
        proxy_buffer_size 32k;
        proxy_busy_buffers_size 64k;
        
        # Extended timeouts
        proxy_connect_timeout 300;
        proxy_send_timeout 300;
        proxy_read_timeout 300;
        
        client_max_body_size 64M;
    }
}

================
File: ansible/roles/wordpress/defaults/main.yml
================
# roles/wordpress/defaults/main.yml
---
wp_namespace: wordpress
wp_domain: test.mmustar.fr
wp_replicas: 1
storage_class_name: ebs-sc
storage_size: 10Gi
wp_memory_request: 256Mi
wp_memory_limit: 512Mi
wp_cpu_request: 250m
wp_cpu_limit: 500m
kubeconfig: /etc/rancher/k3s/k3s.yaml

# Default database variables (seront surchargées par les secrets)
db_host: "{{ db_secrets.MYSQL_HOST | default('localhost') }}"
db_name: "{{ db_secrets.MYSQL_DATABASE | default('wordpress') }}"
db_user: "{{ db_secrets.MYSQL_USER | default('wordpress') }}"
db_password: "{{ db_secrets.MYSQL_PASSWORD | default('change_me') }}"

================
File: ansible/roles/wordpress/tasks/ingress.yml
================
---
- name: Get ingress controller service information
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Service
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: ingress_service

- name: Display ingress service details
  debug:
    var: ingress_service

- name: Apply WordPress Ingress
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('template', '../templates/ingress.yaml.j2') | from_yaml }}"
    namespace: "{{ wp_namespace }}"
  register: ingress_result

- name: Wait for Ingress to be ready
  kubernetes.core.k8s_info:
    api_version: networking.k8s.io/v1
    kind: Ingress
    name: wordpress-ingress
    namespace: "{{ wp_namespace }}"
  register: ingress_status
  until: ingress_status.resources[0].status is defined
  retries: 30
  delay: 10

- name: Display Ingress status
  debug:
    var: ingress_status

================
File: ansible/roles/wordpress/tasks/main.yml
================
---
- name: Ensure kubeconfig directory exists
  file:
    path: "{{ ansible_env.HOME }}/.kube"
    state: directory
    mode: '0755'

- name: Copy kubeconfig locally
  copy:
    src: /etc/rancher/k3s/k3s.yaml
    dest: "{{ ansible_env.HOME }}/.kube/config"
    mode: '0600'
    remote_src: yes

- name: Set KUBECONFIG environment variable
  set_fact:
    kubeconfig: "{{ ansible_env.HOME }}/.kube/config"

- name: Create WordPress namespace
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ wp_namespace }}"

- name: Include secrets tasks
  include_tasks: secrets.yml

- name: Check if WordPress deployment exists
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Deployment
    name: wordpress
    namespace: "{{ wp_namespace }}"
  register: wp_deployment

- name: Scale down WordPress deployment to 0 replicas if it exists
  kubernetes.core.k8s_scale:
    kubeconfig: "{{ kubeconfig }}"
    namespace: "{{ wp_namespace }}"
    kind: Deployment
    name: wordpress
    replicas: 0
  when: wp_deployment.resources | length > 0

- name: Wait for WordPress pods to be terminated
  shell: >
    kubectl --kubeconfig={{ kubeconfig }} get pods -n {{ wp_namespace }} -l app=wordpress --no-headers | wc -l
  register: pod_count
  until: pod_count.stdout | int == 0
  retries: 10
  delay: 5
  when: wp_deployment.resources | length > 0

- name: Delete existing PVC
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: absent
    api_version: v1
    kind: PersistentVolumeClaim
    namespace: "{{ wp_namespace }}"
    name: wp-pvc
  ignore_errors: true

- name: Create PVC and wait for binding
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition: "{{ lookup('template', 'storage.yaml.j2') | from_yaml }}"
    namespace: "{{ wp_namespace }}"
  register: pvc_result

- name: Wait for PVC to be Bound
  shell: >
    kubectl --kubeconfig={{ kubeconfig }} get pvc -n {{ wp_namespace }} wp-pvc -o jsonpath='{.status.phase}'
  register: pvc_status
  until: pvc_status.stdout == "Bound"
  retries: 20
  delay: 5

- name: Create WordPress resources
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    template: "{{ item }}"
    namespace: "{{ wp_namespace }}"
  loop:
    - configmap.yaml.j2
    - secret.yaml.j2
    - deployment.yaml.j2
    - service.yaml.j2

- name: Scale up WordPress deployment to desired replicas
  kubernetes.core.k8s_scale:
    kubeconfig: "{{ kubeconfig }}"
    namespace: "{{ wp_namespace }}"
    resource_definition:
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: wordpress
    replicas: "{{ wp_replicas }}"

- name: Pause 30 seconds to allow pods to start and PVC binding
  pause:
    seconds: 30

- name: Include Ingress tasks
  include_tasks: ingress.yml
  tags:
    - wordpress
    - ingress

- name: Wait for WordPress deployment
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Deployment
    name: wordpress
    namespace: "{{ wp_namespace }}"
  register: wp_deployment
  until: wp_deployment.resources | length > 0 and wp_deployment.resources[0].status.availableReplicas is defined and wp_deployment.resources[0].status.availableReplicas > 0
  retries: 15
  delay: 10

- name: Display WordPress deployment status
  debug:
    var: wp_deployment.resources[0].status

================
File: ansible/roles/wordpress/tasks/secrets.yml
================
# roles/wordpress/tasks/secrets.yml
---
- name: Get AWS secrets
  block:
    - name: Execute get_secret.py
      script: roles/common/files/get_secret.py
      register: secret_output
      delegate_to: localhost
      become: false
      changed_when: false

    - name: Parse JSON output
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true

    - name: Set database variables
      set_fact:
        db_host: "{{ db_secrets.MYSQL_HOST }}"
        db_name: "{{ db_secrets.MYSQL_DATABASE }}"
        db_user: "{{ db_secrets.MYSQL_USER }}"
        db_password: "{{ db_secrets.MYSQL_PASSWORD }}"
      no_log: true

    - name: Verify secrets loaded
      assert:
        that:
          - db_secrets is defined
          - db_secrets.MYSQL_HOST is defined
          - db_secrets.MYSQL_DATABASE is defined
          - db_secrets.MYSQL_USER is defined
          - db_secrets.MYSQL_PASSWORD is defined
        fail_msg: "Required database secrets are missing"
  rescue:
    - name: Debug AWS secret retrieval
      debug:
        msg: "Failed to get AWS secrets: {{ secret_output.stderr if secret_output is defined else 'Unknown error' }}"
      failed_when: true

================
File: ansible/roles/wordpress/templates/configmap.yaml.j2
================
# roles/wordpress/templates/configmap.yaml.j2
apiVersion: v1
kind: ConfigMap
metadata:
  name: wordpress-config
  namespace: {{ wp_namespace }}
data:
  WORDPRESS_CONFIG_EXTRA: |
    define('WP_HOME', 'https://{{ wp_domain }}');
    define('WP_SITEURL', 'https://{{ wp_domain }}');
    define('MYSQL_CLIENT_FLAGS', MYSQLI_CLIENT_SSL);

================
File: ansible/roles/wordpress/templates/deployment.yaml.j2
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  namespace: wordpress
  labels:
    app: wordpress
spec:
  replicas: 1
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
        - name: wordpress
          image: wordpress:latest
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          env:
            - name: WORDPRESS_DB_HOST
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_HOST
            - name: WORDPRESS_DB_USER
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_USER
            - name: WORDPRESS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_PASSWORD
            - name: WORDPRESS_DB_NAME
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_NAME
          volumeMounts:
            - name: wordpress-data
              mountPath: /var/www/html
          readinessProbe:
            httpGet:
              path: /wp-login.php
              port: http
            initialDelaySeconds: 60
            timeoutSeconds: 5
            periodSeconds: 10
            failureThreshold: 3
          livenessProbe:
            httpGet:
              path: /wp-login.php
              port: http
            initialDelaySeconds: 60
            timeoutSeconds: 5
            periodSeconds: 20
            failureThreshold: 6
      volumes:
        - name: wordpress-data
          persistentVolumeClaim:
            claimName: wp-pvc

================
File: ansible/roles/wordpress/templates/ingress.yaml.j2
================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wordpress-ingress
  namespace: {{ wp_namespace }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: "64m"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "32k"
    nginx.ingress.kubernetes.io/proxy-buffers-number: "16"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
    - host: {{ wp_domain }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: wordpress
                port:
                  number: 80

================
File: ansible/roles/wordpress/templates/secret.yaml.j2
================
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-secret
  namespace: {{ wp_namespace }}
type: Opaque
data:
  WORDPRESS_DB_HOST: {{ db_secrets.MYSQL_HOST | b64encode }}
  WORDPRESS_DB_USER: {{ db_secrets.MYSQL_USER | b64encode }}
  WORDPRESS_DB_PASSWORD: {{ db_secrets.MYSQL_PASSWORD | b64encode }}
  WORDPRESS_DB_NAME: {{ db_secrets.MYSQL_DATABASE | b64encode }}

================
File: ansible/roles/wordpress/templates/service.yaml.j2
================
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  namespace: {{ wp_namespace }}
  labels:
    app: wordpress
spec:
  selector:
    app: wordpress
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

================
File: ansible/roles/wordpress/templates/storage.yaml.j2
================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pvc
  namespace: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: ebs-sc

================
File: ansible/ansible.cfg
================
[defaults]
inventory = inventory/hosts.yml
private_key_file = ~/.ssh/test-aws-key-pair-new.pem
host_key_checking = False
remote_user = ubuntu
retry_files_enabled = False
gathering = smart
fact_caching = jsonfile
fact_caching_connection = ~/.ansible/facts_cache
log_path = ~/.ansible/ansible.log
roles_path = roles
nocows = True
remote_tmp = /tmp/.ansible-${USER}/tmp


[ssh_connection]
pipelining = True
ssh_args = -o ControlMaster=auto -o ControlPersist=600s -o ServerAliveInterval=30 -o ServerAliveCountMax=10 -o TCPKeepAlive=yes
control_path = %(directory)s/%%h-%%r
control_path_dir = ~/.ansible/cp
retries = 5
timeout = 60

================
File: ansible/requirements.yml
================
---
collections:
  - name: kubernetes.core
    version: "2.4.0"
  - name: community.general
    version: "7.5.0"

roles: []

================
File: ansible/run.sh
================
# run.sh
#!/bin/bash
set -euo pipefail

# Vérification des variables d'environnement requises
if [[ -z "${AWS_ACCESS_KEY_ID:-}" ]] || [[ -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
    echo "Erreur: Les credentials AWS doivent être définis"
    exit 1
fi

# Vérification de la présence des fichiers nécessaires
required_files=("site.yml" "ansible.cfg" "inventory/hosts.yml")
for file in "${required_files[@]}"; do
    if [[ ! -f "$file" ]]; then
        echo "Erreur: Fichier requis manquant: $file"
        exit 1
    fi
done

# Vérification de la syntaxe
echo "Vérification de la syntaxe du playbook..."
if ! ansible-playbook --syntax-check site.yml; then
    echo "Erreur: Le playbook contient des erreurs de syntaxe"
    exit 1
fi

# Exécution du playbook
echo "Exécution du playbook..."
ansible-playbook site.yml -v

# Vérification du statut de sortie
if [[ $? -eq 0 ]]; then
    echo "Le playbook s'est exécuté avec succès"
else
    echo "Erreur lors de l'exécution du playbook"
    exit 1
fi

================
File: ansible/site.yml
================
- name: Configuration complète des serveurs WordPress
  hosts: wordpress
  become: true
  vars:
    aws_region: eu-west-3
    ebs_csi_enabled: true
    nginx_server_name: test.mmustar.fr
    domain_name: test.mmustar.fr
    wp_namespace: wordpress
    kubeconfig: /etc/rancher/k3s/k3s.yaml

  pre_tasks:
    - name: Installer net-tools
      apt:
        name: net-tools
        state: present
      tags:
        - net-tools

    - name: Get AWS secret
      block:
        - script: roles/common/files/get_secret.py
          register: secret_output
          delegate_to: localhost
          become: false
          changed_when: false
      rescue:
        - fail:
            msg: "Impossible de récupérer les secrets AWS. Vérifiez vos credentials AWS et l'existence du secret."

    - name: Set database secrets
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true
      when: secret_output is defined

    - name: Debug db_secrets
      debug:
        msg: "DB Secrets loaded: {{ db_secrets is defined }}"
      when: secret_output is defined

  roles:
    - { role: common, tags: ['common'] }
    - { role: docker, tags: ['docker'] }
    - { role: k3s, tags: ['k3s'] }
    - { role: wordpress, tags: ['wordpress'] }
    - { role: nginx, tags: ['nginx'] }

================
File: environments/backend-config/.terraform.lock.hcl
================
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.0.0"
  constraints = "~> 5.0"
  hashes = [
    "h1:swP2uqDPi7bRLe+J4oUGEp8ZPTG4NaAV9QK+Iqgo2ro=",
  ]
}

================
File: environments/backend-config/main.tf
================
# environments/backend-config/main.tf

# Un seul bucket S3 pour tous les environnements
resource "aws_s3_bucket" "terraform_state" {
  bucket = "${var.project_name}-terraform-state"

  lifecycle {
    prevent_destroy = true
  }

  tags = {
    Name    = "${var.project_name}-terraform-state"
    Project = var.project_name
  }
}

# Activation du versioning
resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

# Activation du chiffrement
resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

# Blocage de l'accès public
resource "aws_s3_bucket_public_access_block" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Une seule table DynamoDB pour le verrouillage
resource "aws_dynamodb_table" "terraform_locks" {
  name         = "${var.project_name}-terraform-locks"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  tags = {
    Name    = "${var.project_name}-terraform-locks"
    Project = var.project_name
  }
}

================
File: environments/backend-config/outputs.tf
================
# environments/backend-config/outputs.tf
output "bucket_name" {
  description = "Nom du bucket S3 pour le state Terraform"
  value       = aws_s3_bucket.terraform_state.id
}

output "dynamodb_table_name" {
  description = "Nom de la table DynamoDB pour le verrouillage"
  value       = aws_dynamodb_table.terraform_locks.name
}

================
File: environments/backend-config/variables.tf
================
# environments/backend-config/variables.tf
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "eu-west-3"
}

variable "project_name" {
  description = "Nom du projet"
  type        = string
  default     = "wordpress-mmustar"
}

================
File: environments/backend-config/versions.tf
================
# environments/backend-config/versions.tf
terraform {
  required_version = ">= 1.0.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = var.project_name
      ManagedBy   = "terraform"
    }
  }
}

================
File: environments/modules/compute/main.tf
================
# environments/modules/compute/main.tf
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "wordpress" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = var.instance_type
  subnet_id     = data.aws_subnet.rds_subnet.id
  vpc_security_group_ids = [aws_security_group.wordpress.id]
  key_name      = var.key_name


  root_block_device {
    volume_size = 20
    volume_type = "gp3"
  }

  tags = {
    Name        = "${var.project_name}-instance-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    prevent_destroy = true
  }
}

resource "aws_eip" "wordpress" {
  count    = var.environment == "test" ? 1 : 0
  instance = aws_instance.wordpress.id
  domain   = "vpc"

  tags = {
    Name        = "${var.project_name}-eip-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}
data "aws_vpc" "rds_vpc" {
  id = "vpc-0385cddb5bd815883"
}

data "aws_subnet" "rds_subnet" {
  vpc_id = data.aws_vpc.rds_vpc.id
}

================
File: environments/modules/compute/outputs.tf
================
# environments/modules/compute/outputs.tf
output "instance_id" {
  description = "ID of the created EC2 instance"
  value       = aws_instance.wordpress.id
}

output "instance_public_ip" {
  description = "Public IP of the EC2 instance"
  value       = var.environment == "test" ? aws_eip.wordpress[0].public_ip : aws_instance.wordpress.public_ip
}

output "instance_private_ip" {
  description = "Private IP of the EC2 instance"
  value       = aws_instance.wordpress.private_ip
}

================
File: environments/modules/compute/variables.tf
================
# environments/modules/compute/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the instances will be created"
  type        = string
}

variable "subnet_id" {
  description = "Subnet ID where the instances will be created"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t2.micro"
}

variable "key_name" {
  description = "Name of the SSH key pair"
  type        = string
}

================
File: environments/modules/k3s/iam.tf
================
# modules/k3s/iam.tf
resource "aws_iam_openid_connect_provider" "k3s" {
  url             = "https://token.actions.githubusercontent.com"
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = ["1b511abead59c6ce207077c0bf0e0043b1382612"]
}

resource "aws_iam_role" "ebs_csi" {
  name = "${var.environment}-ebs-csi"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = aws_iam_openid_connect_provider.k3s.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${aws_iam_openid_connect_provider.k3s.url}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
      }
    }]
  })
}

resource "aws_iam_role_policy" "ebs_csi" {
  name = "${var.environment}-ebs-csi-policy"
  role = aws_iam_role.ebs_csi.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ec2:CreateSnapshot",
          "ec2:AttachVolume",
          "ec2:DetachVolume",
          "ec2:ModifyVolume",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeInstances",
          "ec2:DescribeSnapshots",
          "ec2:DescribeTags",
          "ec2:DescribeVolumes",
          "ec2:DescribeVolumesModifications",
          "ec2:CreateVolume",
          "ec2:DeleteVolume",
          "ec2:CreateTags",
          "ec2:DeleteTags"
        ]
        Resource = "*"
      }
    ]
  })
}

variable "environment" {
  description = "Environment name"
  type        = string
}

output "ebs_csi_role_arn" {
  value = aws_iam_role.ebs_csi.arn
}

================
File: environments/modules/network/main.tf
================
data "aws_availability_zones" "available" {
  state = "available"
}

resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "${var.project_name}-vpc-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name        = "${var.project_name}-igw-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_subnet" "public" {
  count                   = length(var.public_subnet_cidrs)
  vpc_id                  = aws_vpc.main.id
  cidr_block              = var.public_subnet_cidrs[count.index]
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name        = "${var.project_name}-public-subnet-${count.index + 1}-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name        = "${var.project_name}-public-rt-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/network/outputs.tf
================
# environments/modules/network/outputs.tf
output "vpc_id" {
  value = aws_vpc.main.id
}

output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "route_table_id" {
  value = aws_route_table.public.id
}

================
File: environments/modules/network/security.tf
================
# environments/modules/network/security.tf
resource "aws_security_group" "wordpress" {
  name_prefix = "${var.project_name}-sg-${var.environment}"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]  # À restreindre selon vos besoins
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "${var.project_name}-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/network/variables.tf
================
variable "project_name" {
  description = "Nom du projet"
  type        = string
}

variable "environment" {
  description = "Environnement (test, prod)"
  type        = string
}

variable "vpc_cidr" {
  description = "CIDR block du VPC"
  type        = string
}

variable "public_subnet_cidrs" {
  description = "CIDR des subnets publics"
  type        = list(string)
}

variable "rds_vpc_id" {
  description = "ID du VPC contenant le RDS"
  type        = string
}

variable "rds_cidr_block" {
  description = "Plage CIDR du VPC RDS"
  type        = string
}

variable "rds_route_table_id" {
  description = "ID de la table de routage du VPC contenant le RDS"
  type        = string
}

variable "rds_security_group_id" {
  description = "ID du Security Group du RDS"
  type        = string
}

variable "ec2_vpc_id" {
  description = "ID du VPC de l'EC2"
  type        = string
}

variable "ec2_cidr_block" {
  description = "Plage CIDR du VPC de l'EC2"
  type        = string
}

variable "route_table_id" {
  description = "ID de la table de routage du VPC de l'EC2"
  type        = string
}

================
File: environments/modules/network/vpc-peering.tf
================
resource "aws_vpc_peering_connection" "ec2_to_rds" {
  vpc_id      = var.ec2_vpc_id
  peer_vpc_id = var.rds_vpc_id
  auto_accept = false

  tags = {
    Name = "${var.project_name}-ec2-to-rds-peering-${var.environment}"
  }
}

resource "aws_route" "ec2_to_rds_route" {
  route_table_id         = var.route_table_id  # Correction ici !
  destination_cidr_block = var.rds_cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.ec2_to_rds.id
}

resource "aws_route" "rds_to_ec2_route" {
  route_table_id         = var.rds_route_table_id
  destination_cidr_block = var.ec2_cidr_block
  vpc_peering_connection_id = aws_vpc_peering_connection.ec2_to_rds.id
}

resource "aws_security_group_rule" "allow_mysql_peering" {
  type        = "ingress"
  from_port   = 3306
  to_port     = 3306
  protocol    = "tcp"
  security_group_id = var.rds_security_group_id
  cidr_blocks = [var.ec2_cidr_block]
}

================
File: environments/modules/security/main.tf
================
# environments/modules/security/main.tf
resource "aws_security_group" "wordpress" {
  name_prefix = "${var.project_name}-wp-${var.environment}"
  description = "Security group for WordPress instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTP"
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTPS"
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # À restreindre selon vos besoins
    description = "Allow SSH"
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow all outbound traffic"
  }

  tags = {
    Name        = "${var.project_name}-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_security_group" "rds" {
  name_prefix = "${var.project_name}-rds-${var.environment}"
  description = "Security group for RDS instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = "tcp"
    security_groups = [aws_security_group.wordpress.id]
    description     = "Allow MySQL from WordPress SG"
  }

  tags = {
    Name        = "${var.project_name}-rds-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/security/outputs.tf
================
# environments/modules/security/outputs.tf
output "wordpress_sg_id" {
  description = "ID of WordPress security group"
  value       = aws_security_group.wordpress.id
}

output "rds_sg_id" {
  description = "ID of RDS security group"
  value       = aws_security_group.rds.id
}

================
File: environments/modules/security/variables.tf
================
# environments/modules/security/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the security groups will be created"
  type        = string
}

================
File: environments/prod/backend.tf
================
# environments/prod/backend.tf
terraform {
  backend "s3" {
    bucket         = "wordpress-mmustar-terraform-state"
    key            = "environments/prod/terraform.tfstate"
    region         = "eu-west-3"
    dynamodb_table = "wordpress-mmustar-terraform-locks"
    encrypt        = true
  }
}

================
File: environments/test/.terraform.lock.hcl
================
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.0.0"
  constraints = "~> 5.0"
  hashes = [
    "h1:swP2uqDPi7bRLe+J4oUGEp8ZPTG4NaAV9QK+Iqgo2ro=",
  ]
}

================
File: environments/test/backend.tf
================
# environments/test/backend.tf
terraform {
  backend "s3" {
    bucket         = "wordpress-mmustar-terraform-state"
    key            = "test/terraform.tfstate"
    region         = "eu-west-3"
    dynamodb_table = "wordpress-mmustar-terraform-locks"
    encrypt        = true
  }
}

================
File: environments/test/main.tf
================
# Provider configuration
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  required_version = ">= 1.0.0"
}

provider "aws" {
  region = "eu-west-3"
}

# Variables
variable "project_name" {
  description = "The name of the project"
  type        = string
}

variable "environment" {
  description = "The environment for deployment"
  type        = string
}

# Define the VPC
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"

  tags = {
    Name = "main-vpc"
  }
}

# Define the availability zones data source
data "aws_availability_zones" "available" {
  state = "available"
}

# Modules
module "network" {
  source = "../modules/network"

  project_name        = var.project_name
  environment         = var.environment
  vpc_cidr            = var.vpc_cidr
  public_subnet_cidrs = var.public_subnet_cidrs

  ec2_vpc_id            = var.ec2_vpc_id
  ec2_cidr_block        = var.ec2_cidr_block
  rds_vpc_id            = var.rds_vpc_id
  rds_cidr_block        = var.rds_cidr_block
  rds_route_table_id    = var.rds_route_table_id
  rds_security_group_id = var.rds_security_group_id
  route_table_id        = module.network.route_table_id # Ajout ici !
}

module "k3s" {
  source      = "../modules/k3s"
  environment = var.environment
}

# Security Group
resource "aws_security_group" "wordpress_test" {
  name_prefix = "WP-SecurityGroup-Test-"
  description = "Security group for WordPress test instance"
  vpc_id      = module.network.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "WP-SG-Test"
    Environment = var.environment
    Project     = var.project_name
  }

  depends_on = [module.network]
}

# Existing EIP
data "aws_eip" "wordpress_test" {
  id = "eipalloc-0933b219497dd6c15"
}

# Data source pour le rôle IAM existant
data "aws_iam_role" "ec2_secrets_manager_role" {
  name = "EC2SecretsManagerRole"
}

# IAM Instance Profile pour EC2
resource "aws_iam_instance_profile" "ec2_secrets_manager_profile" {
  name = "ec2-secrets-manager-profile"
  role = data.aws_iam_role.ec2_secrets_manager_role.name
}

# EC2 Instance
resource "aws_instance" "wordpress_test" {
  ami                         = "ami-06e02ae7bdac6b938"
  instance_type               = "t3.medium"
  subnet_id                   = module.network.public_subnet_ids[0]
  vpc_security_group_ids      = [aws_security_group.wordpress_test.id]
  key_name                    = "test-aws-key-pair-new"
  associate_public_ip_address = true
  iam_instance_profile        = aws_iam_instance_profile.ec2_secrets_manager_profile.name

  root_block_device {
    volume_size           = 8
    volume_type           = "gp3"
    delete_on_termination = true
  }

  tags = {
    Name        = "WP-Instance-Test"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    create_before_destroy = true
  }

  depends_on = [
    module.network,
    aws_security_group.wordpress_test,
    aws_iam_instance_profile.ec2_secrets_manager_profile
  ]
}

# EIP Association
resource "aws_eip_association" "wordpress_test" {
  instance_id   = aws_instance.wordpress_test.id
  allocation_id = data.aws_eip.wordpress_test.id
  depends_on    = [aws_instance.wordpress_test]
}

# RDS Security Group and Instance data sources
data "aws_security_group" "rds" {
  id = "sg-00efe258e85b22a30"
}

# Define the RDS instance data source
data "aws_db_instance" "wordpress" {
  db_instance_identifier = "wordpress-db"
}

# Outputs
output "rds_endpoint" {
  value = data.aws_db_instance.wordpress.endpoint
}

output "instance_public_ip" {
  value = aws_instance.wordpress_test.public_ip
}

output "instance_id" {
  value = aws_instance.wordpress_test.id
}

output "eip_public_ip" {
  value = data.aws_eip.wordpress_test.public_ip
}

output "ebs_csi_role_arn" {
  value = module.k3s.ebs_csi_role_arn
}

output "public_subnet_ids" {
  value = module.network.public_subnet_ids
}

================
File: environments/test/variables.tf
================
# environments/test/variables.tf
variable "aws_region" {
  description = "AWS Region"
  type        = string
  default     = "eu-west-3"
}

# SSH key pour instance EC2
variable "key_name" {
  description = "SSH key pair name"
  type        = string
  default     = "test-aws-key-pair-new" # Remplacez par votre nom de clé
}

variable "vpc_cidr" {
  description = "CIDR block pour le VPC"
  type        = string
}

variable "public_subnet_cidrs" {
  description = "CIDR des subnets publics"
  type        = list(string)
}

variable "rds_vpc_id" {
  description = "ID du VPC du RDS"
  type        = string
}

variable "rds_cidr_block" {
  description = "Plage CIDR du VPC RDS"
  type        = string
}

variable "rds_route_table_id" {
  description = "ID de la table de routage du VPC du RDS"
  type        = string
}

variable "rds_security_group_id" {
  description = "ID du Security Group du RDS"
  type        = string
}

variable "ec2_vpc_id" {
  description = "ID du VPC contenant l'EC2"
  type        = string
}

variable "ec2_cidr_block" {
  description = "CIDR block du VPC de l'EC2"
  type        = string
}

variable "route_table_id" {
  description = "ID de la table de routage du VPC de l'EC2"
  type        = string
}

================
File: scripts/ssh-update.sh
================
#!/bin/bash

# Configuration par défaut
SSH_KEY_PATH="/home/gnou/.ssh/test-aws-key-pair-new.pem"
EC2_USER="ubuntu"
KNOWN_HOSTS="/home/gnou/.ssh/known_hosts"

# Fonction d'aide
show_usage() {
    echo "Usage: $0 [OPTIONS] EC2_IP_ADDRESS"
    echo "Options:"
    echo "  -k, --key PATH    Chemin vers la clé SSH (default: $SSH_KEY_PATH)"
    echo "  -u, --user USER   Utilisateur EC2 (default: $EC2_USER)"
    echo "  -h, --help        Affiche cette aide"
    exit 1
}

# Traitement des arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -k|--key)
            SSH_KEY_PATH="$2"
            shift 2
            ;;
        -u|--user)
            EC2_USER="$2"
            shift 2
            ;;
        -h|--help)
            show_usage
            ;;
        *)
            EC2_IP="$1"
            shift
            ;;
    esac
done

# Vérification des paramètres requis
if [ -z "$EC2_IP" ]; then
    echo "Erreur: Adresse IP de l'instance EC2 manquante"
    show_usage
fi

if [ ! -f "$SSH_KEY_PATH" ]; then
    echo "Erreur: Clé SSH non trouvée: $SSH_KEY_PATH"
    exit 1
fi

# Fonction pour vérifier si l'instance est accessible
check_instance() {
    timeout 5 nc -zv $EC2_IP 22 &>/dev/null
    return $?
}

# Attendre que l'instance soit accessible
echo "Vérification de l'accessibilité de l'instance..."
ATTEMPTS=0
MAX_ATTEMPTS=10

while ! check_instance; do
    ATTEMPTS=$((ATTEMPTS + 1))
    if [ $ATTEMPTS -ge $MAX_ATTEMPTS ]; then
        echo "Erreur: Impossible de se connecter à l'instance après $MAX_ATTEMPTS tentatives"
        exit 1
    fi
    echo "Instance non accessible, nouvelle tentative dans 5 secondes... ($ATTEMPTS/$MAX_ATTEMPTS)"
    sleep 5
done

# Supprimer l'ancienne clé host si elle existe
if ssh-keygen -F $EC2_IP >/dev/null 2>&1; then
    echo "Suppression de l'ancienne clé host..."
    ssh-keygen -f "$KNOWN_HOSTS" -R "$EC2_IP" >/dev/null 2>&1
fi

# Tentative de connexion
echo "Connexion à l'instance $EC2_IP..."
ssh -o StrictHostKeyChecking=accept-new \
    -o ConnectTimeout=10 \
    -i "$SSH_KEY_PATH" \
    "$EC2_USER@$EC2_IP"

exit_code=$?
if [ $exit_code -ne 0 ]; then
    echo "Erreur lors de la connexion (code: $exit_code)"
    exit $exit_code
fi

================
File: -n ingress-nginx
================
{
    "SecurityGroups": [
        {
            "Description": "Security group for WordPress",
            "GroupName": "WP-SecurityGroup",
            "IpPermissions": [
                {
                    "FromPort": 80,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 80,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 30000,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 32767,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 31097,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 31097,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 22,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 22,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 443,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 443,
                    "UserIdGroupPairs": []
                }
            ],
            "OwnerId": "730335289383",
            "GroupId": "sg-0e2ab25ac27f8bbc5",
            "IpPermissionsEgress": [
                {
                    "IpProtocol": "-1",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "UserIdGroupPairs": []
                }
            ],
            "Tags": [
                {
                    "Key": "book",
                    "Value": "book"
                }
            ],
            "VpcId": "vpc-0385cddb5bd815883"
        }
    ]
}

================
File: .gitignore
================
# Local .terraform directories
**/.terraform/*

# .tfstate files
*.tfstate
*.tfstate.*

# Crash log files
crash.log
crash.*.log

# Exclude all .tfvars files, which are likely to contain sensitive data
*.tfvars
*.tfvars.json

# Ignore override files as they are usually used to override resources locally
override.tf
override.tf.json
*_override.tf
*_override.tf.json

# Ignore CLI configuration files
.terraformrc
terraform.rc

# Ignore any .env files
*.env

# Ignore local development files
.vscode/
.idea/

================
File: repomix-output-1.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-03T18:28:21.465Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
ansible/
  group_vars/
    all.yml
  inventory/
    hosts.yml
  roles/
    common/
      defaults/
        main.yml
      files/
        get_secret.py
      handlers/
        main.yml
      tasks/
        main.yml
    docker/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        main.yml
    k3s/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        ebs-csi.yml
        helm.yml
        main.yml
      templates/
        ebs-csi-values.yml.j2
    nginx/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        main.yml
      templates/
        wordpress.conf.j2
    wordpress/
      defaults/
        main.yml
      tasks/
        ingress.yml
        main.yml
        secrets.yml
      templates/
        configmap.yaml.j2
        deployment.yaml.j2
        ingress.yaml.j2
        secret.yaml.j2
        service.yaml.j2
        storage.yaml.j2
        wordpress.conf.j2
      vars/
        main.yml
  ansible.cfg
  repomix-output.txt
  requirements.yml
  run.sh
  site.yml
environments/
  backend-config/
    .terraform.lock.hcl
    main.tf
    outputs.tf
    variables.tf
    versions.tf
  modules/
    compute/
      main.tf
      outputs.tf
      variables.tf
    k3s/
      iam.tf
    network/
      main.tf
      outputs.tf
      security.tf
      variables.tf
    security/
      main.tf
      outputs.tf
      variables.tf
  prod/
    backend.tf
  test/
    .terraform.lock.hcl
    backend.tf
    main.tf
    variables.tf
scripts/
  ssh-update.sh
-n ingress-nginx
.gitignore

================================================================
Files
================================================================

================
File: ansible/group_vars/all.yml
================
# ansible/group_vars/all.yml
---
# Variables globales communes à tous les serveurs
timezone: UTC
environment: test
aws_region: eu-west-3
ebs_csi_enabled: true
cloud_provider: aws
k8s_storage_class: ebs-sc

================
File: ansible/inventory/hosts.yml
================
all:
  vars:
    ansible_python_interpreter: /usr/bin/python3
    pip_package: python3-pip
    pip_install_packages:
      - name: boto3
  children:
    wordpress:
      hosts:
        wp-test:
          ansible_host: 35.180.222.29
          ansible_user: ubuntu
          ansible_ssh_private_key_file: ~/.ssh/test-aws-key-pair-new.pem

================
File: ansible/roles/common/defaults/main.yml
================
# roles/common/defaults/main.yml
---
system_packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - software-properties-common
  - python3-pip
  - python3-boto3
  - python3-botocore
  - nfs-common
  - git
  - vim
timezone: UTC
environment: test

================
File: ansible/roles/common/files/get_secret.py
================
#!/usr/bin/env python3
import boto3
import json
import sys

try:
    client = boto3.client('secretsmanager', region_name='eu-west-3')
    response = client.get_secret_value(SecretId='book')
    print(response['SecretString'])
except Exception as e:
    print(f"Erreur: {str(e)}", file=sys.stderr)
    sys.exit(1)

================
File: ansible/roles/common/handlers/main.yml
================
# roles/common/handlers/main.yml
---
- name: restart systemd-resolved
  service:
    name: systemd-resolved
    state: restarted

================
File: ansible/roles/common/tasks/main.yml
================
---
- name: Wait for cloud-init to complete  
  command: cloud-init status --wait  
  register: cloud_init_result  
  until: cloud_init_result.rc == 0  
  retries: 30  
  delay: 10  
  changed_when: false
  
- name: Reset connection to allow group changes to take effect  
  meta: reset_connection

- name: Wait for apt lock  
  shell: while sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1 || sudo fuser /var/lib/apt/lists/lock >/dev/null 2>&1; do sleep 1; done  
  changed_when: false
  
- name: Clean apt  
  apt:
    clean: yes  
    autoclean: yes  
    autoremove: yes  
    force_apt_get: yes  
  become: true

- name: Force kill apt/dpkg processes  
  shell: |
    killall apt apt-get dpkg 2>/dev/null || true  
    rm -f /var/lib/apt/lists/lock /var/cache/apt/archives/lock /var/lib/dpkg/lock* 2>/dev/null || true  
    dpkg --configure -a  
  changed_when: false  
  ignore_errors: true  
  become: true

- name: Update package cache  
  apt:
    update_cache: yes  
    cache_valid_time: 3600  
    force_apt_get: yes  
  register: apt_update  
  retries: 5  
  delay: 10  
  until: apt_update is success  
  become: true

- name: Install system packages  
  apt:
    name:
      - python3  
      - python3-pip  
      - python3-boto3  
      - python3-botocore  
      - python3-kubernetes  
      - python3-openshift  
      - git  
      - vim  
    state: present  
    force_apt_get: yes  
    update_cache: yes  
  become: true  
  register: pkg_install  
  retries: 3  
  delay: 10  
  until: pkg_install is success

================
File: ansible/roles/docker/defaults/main.yml
================
# roles/docker/defaults/main.yml
---
docker_users:
  - "{{ ansible_user }}"

================
File: ansible/roles/docker/handlers/main.yml
================
# roles/docker/handlers/main.yml
---
- name: restart docker
  service:
    name: docker
    state: restarted

================
File: ansible/roles/docker/tasks/main.yml
================
- name: Install required packages
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg-agent
      - software-properties-common
    state: present

- name: Add Docker GPG key
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: Add Docker repository
  apt_repository:
    repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable
    state: present

- name: Install Docker
  apt:
    name:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - docker-compose-plugin
    state: present
    update_cache: yes

- name: Start and enable Docker
  service:
    name: docker
    state: started
    enabled: yes

- name: Add users to docker group
  user:
    name: "{{ item }}"
    groups: docker
    append: yes
  with_items: "{{ docker_users }}"

- name: Configure Docker daemon
  copy:
    content: |
      {
        "log-driver": "json-file",
        "log-opts": {
          "max-size": "100m",
          "max-file": "3"
        }
      }
    dest: /etc/docker/daemon.json
  notify: restart docker

================
File: ansible/roles/k3s/defaults/main.yml
================
# roles/k3s/defaults/main.yml
---
k3s_version: "v1.28.4+k3s2"
k3s_server_args: "--disable traefik --disable servicelb"
ebs_csi_enabled: true
ebs_csi_version: "2.26.0"
storage_class_name: "ebs-sc"
aws_region: "eu-west-3"
wp_namespace: wordpress
kubeconfig: /etc/rancher/k3s/k3s.yaml
security_group_id: "sg-0e2ab25ac27f8bbc5"
project_name: "wordpress-mmustar"
environment: "test"

================
File: ansible/roles/k3s/handlers/main.yml
================
# roles/k3s/handlers/main.yml
---
- name: restart k3s
  service:
    name: k3s
    state: restarted

- name: wait for k3s
  wait_for:
    path: "{{ kubeconfig }}"
    delay: 10
    timeout: 300

================
File: ansible/roles/k3s/tasks/ebs-csi.yml
================
# roles/k3s/tasks/ebs-csi.yml
---
- name: Fetch AWS Secrets
  command: "aws secretsmanager get-secret-value --secret-id book --region {{ aws_region }} --output json"
  register: secrets_result
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set variables from secrets
  set_fact:
    aws_account_id: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_account_id') }}"
    aws_ebs_csi_role_name: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_ebs_csi_role_name') }}"

- name: Check if EBS CSI Driver is already installed
  command: helm list -n kube-system
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_list
  changed_when: false

- name: Delete old StorageClass if exists
  command: kubectl delete storageclass ebs-sc
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  ignore_errors: true
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Add EBS CSI Helm repository
  kubernetes.core.helm_repository:
    name: aws-ebs-csi-driver
    repo_url: https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    state: present
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Update Helm repositories
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

- name: Template EBS CSI values
  template:
    src: ebs-csi-values.yml.j2
    dest: /tmp/ebs-csi-values.yml
    mode: '0644'
  when: ebs_csi_enabled

- name: Create kube-system namespace if not exists
  kubernetes.core.k8s:
    name: kube-system
    api_version: v1
    kind: Namespace
    state: present
    kubeconfig: /etc/rancher/k3s/k3s.yaml

- name: Install AWS EBS CSI Driver
  kubernetes.core.helm:
    name: aws-ebs-csi-driver
    chart_ref: aws-ebs-csi-driver/aws-ebs-csi-driver
    release_namespace: kube-system
    create_namespace: true
    values_files:
      - /tmp/ebs-csi-values.yml
    chart_version: "{{ ebs_csi_version }}"
    wait: true
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Wait for EBS CSI Driver pods
  shell: >
    kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get pods -n kube-system 
    -l app.kubernetes.io/name=aws-ebs-csi-driver --no-headers
  register: ebs_csi_pods
  until: ebs_csi_pods.rc == 0 and ebs_csi_pods.stdout_lines | length > 0 and ebs_csi_pods.stdout.find('Running') != -1
  retries: 30
  delay: 10
  when: ebs_csi_enabled

- name: Clean up temporary values file
  file:
    path: /tmp/ebs-csi-values.yml
    state: absent
  when: ebs_csi_enabled

================
File: ansible/roles/k3s/tasks/helm.yml
================
# roles/k3s/tasks/helm.yml
---
- name: Add Helm apt key
  apt_key:
    url: https://baltocdn.com/helm/signing.asc
    state: present
  become: true

- name: Add Helm repository
  apt_repository:
    repo: deb https://baltocdn.com/helm/stable/debian/ all main
    state: present
    filename: helm
  become: true

- name: Install Helm
  apt:
    name: helm
    state: present
    update_cache: yes
  become: true
  register: helm_install
  retries: 3
  delay: 5
  until: helm_install is success

- name: Verify Helm installation
  command: helm version
  register: helm_version
  changed_when: false
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Add stable Helm repository
  command: helm repo add stable https://charts.helm.sh/stable
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true
  failed_when: false

- name: Update Helm repos
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

================
File: ansible/roles/k3s/tasks/main.yml
================
# roles/k3s/tasks/main.yml
---
# Étape 1: Configuration initiale
- name: Get subnet IDs from AWS
  shell: >
    aws ec2 describe-subnets
    --region {{ aws_region }}
    --filters "Name=tag:Project,Values={{ project_name }}" "Name=tag:Environment,Values={{ environment }}"
    --query "Subnets[0].SubnetId"
    --output text
  register: subnet_info
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set subnet ID
  set_fact:
    public_subnet_id: "{{ subnet_info.stdout }}"
  when: subnet_info.stdout != ""

# Étape 2: Installation de K3s
- name: Download k3s installation script
  get_url:
    url: https://get.k3s.io
    dest: /tmp/k3s-install.sh
    mode: '0755'

- name: Install k3s
  shell: 
    cmd: INSTALL_K3S_VERSION={{ k3s_version }} /tmp/k3s-install.sh --write-kubeconfig-mode 644 {{ k3s_server_args }}
  args:
    creates: /usr/local/bin/k3s
  environment:
    INSTALL_K3S_EXEC: "{{ k3s_server_args }}"
  register: k3s_install

- name: Wait for k3s service to start
  systemd:
    name: k3s
    state: started
    enabled: yes
  register: k3s_service

- name: Configure kubeconfig permissions
  file:
    path: /etc/rancher/k3s/k3s.yaml
    mode: '0644'
  become: true

- name: Set KUBECONFIG environment variable
  lineinfile:
    path: /home/{{ ansible_user }}/.bashrc
    line: "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml"
    state: present
  become: true

# Étape 3: Vérification de l'installation
- name: Wait for k3s node to be ready
  shell: kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes
  register: node_status
  until: "'Ready' in node_status.stdout"
  retries: 30
  delay: 10
  become: true

# Étape 4: Configuration du Security Group
- name: Add NodePort range to security group
  shell: >
    aws ec2 authorize-security-group-ingress 
    --region {{ aws_region }}
    --group-id {{ security_group_id }}
    --protocol tcp 
    --port 30000-32767 
    --cidr 0.0.0.0/0
  delegate_to: localhost
  become: false
  register: sg_result
  failed_when: 
    - sg_result.rc != 0 
    - '"InvalidPermission.Duplicate" not in sg_result.stderr'
  changed_when: sg_result.rc == 0

# Étape 5: Installation de Helm
- name: Include Helm installation tasks
  include_tasks: helm.yml

# Étape 6: Installation de NGINX Ingress
- name: Add ingress-nginx repository
  command: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true
  ignore_errors: true

- name: Update Helm repos
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

- name: Install ingress-nginx with Helm
  kubernetes.core.helm:
    name: ingress-nginx
    chart_ref: ingress-nginx/ingress-nginx
    release_namespace: ingress-nginx
    create_namespace: true
    release_values:
      controller:
        service:
          type: NodePort
          nodePorts:
            http: 32569
            https: 32570
        kind: DaemonSet
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  register: nginx_helm_result

- name: Wait for ingress-nginx pods to be Running
  shell: >
    kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller --no-headers
  register: nginx_pod_status
  until: nginx_pod_status.rc == 0 and 'Running' in nginx_pod_status.stdout
  retries: 30
  delay: 10

- name: Display ingress-nginx pod logs if not running
  shell: kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=50
  register: nginx_logs
  when: "'Running' not in nginx_pod_status.stdout"

- name: Show pod logs
  debug:
    var: nginx_logs.stdout_lines
  when: nginx_logs is defined

# Étape 7: Installation du EBS CSI Driver
- name: Include EBS CSI tasks
  include_tasks: ebs-csi.yml
  when: ebs_csi_enabled

# Étape 8: Vérification finale
- name: Verify cluster status
  command: kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes,pods --all-namespaces
  register: cluster_status
  changed_when: false

- name: Show cluster status
  debug:
    var: cluster_status.stdout_lines

================
File: ansible/roles/k3s/templates/ebs-csi-values.yml.j2
================
# roles/k3s/templates/ebs-csi-values.yml.j2
controller:
  region: "{{ aws_region }}"
  serviceAccount:
    create: true
    name: ebs-csi-controller-sa
    annotations:
      eks.amazonaws.com/role-arn: null
      
node:
  serviceAccount:
    create: true
    name: ebs-csi-node-sa

storageClasses:
- name: ebs-sc
  provisioner: ebs.csi.aws.com
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  volumeBindingMode: WaitForFirstConsumer
  allowVolumeExpansion: true
  reclaimPolicy: Delete
  parameters:
    type: gp3

================
File: ansible/roles/nginx/defaults/main.yml
================
domain_name: "test.mmustar.fr"
cert_email: "mmustar@free.fr"
ingress_http_nodeport: 32569

================
File: ansible/roles/nginx/handlers/main.yml
================
---
- name: Reload Nginx
  service:
    name: nginx
    state: reloaded
  become: true

- name: Restart Nginx
  service:
    name: nginx
    state: restarted
  become: true

- name: Enable Nginx
  service:
    name: nginx
    enabled: true
  become: true

- name: Test Nginx configuration
  command: nginx -t
  become: true
  register: nginx_config_test
  changed_when: false

================
File: ansible/roles/nginx/tasks/main.yml
================
---
# Installation de Nginx
- name: Install Nginx
  apt:
    name: nginx
    state: present
    update_cache: yes
  become: true

# Récupérer le NodePort de l'ingress
- name: Get ingress NodePort
  kubernetes.core.k8s_info:
    kubeconfig: /etc/rancher/k3s/k3s.yaml
    api_version: v1
    kind: Service
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: ingress_svc

- name: Set ingress port fact
  set_fact:
    ingress_http_nodeport: "{{ ingress_svc.resources[0].spec.ports | selectattr('name', 'equalto', 'http') | map(attribute='nodePort') | first }}"

# Configuration Nginx pour WordPress
- name: Create wordpress.conf
  template:
    src: wordpress.conf.j2
    dest: /etc/nginx/sites-available/wordpress.conf
    mode: '0644'
  become: true
  notify: Reload Nginx

- name: Enable wordpress site
  file:
    src: /etc/nginx/sites-available/wordpress.conf
    dest: /etc/nginx/sites-enabled/wordpress.conf
    state: link
  become: true
  notify: Reload Nginx

- name: Remove default nginx site
  file:
    path: /etc/nginx/sites-enabled/default
    state: absent
  become: true
  notify: Reload Nginx

- name: Ensure Nginx is started and enabled
  service:
    name: nginx
    state: started
    enabled: yes
  become: true

================
File: ansible/roles/nginx/templates/wordpress.conf.j2
================
server {
    listen 80;
    server_name {{ domain_name }};
    
    location / {
        proxy_pass http://127.0.0.1:{{ ingress_http_nodeport }};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Increased buffer settings
        proxy_buffers 16 16k;  
        proxy_buffer_size 32k;
        proxy_busy_buffers_size 64k;
        
        # Extended timeouts
        proxy_connect_timeout 300;
        proxy_send_timeout 300;
        proxy_read_timeout 300;
        
        client_max_body_size 64M;
    }
}

================
File: ansible/roles/wordpress/defaults/main.yml
================
# roles/wordpress/defaults/main.yml
---
wp_namespace: wordpress
wp_domain: test.mmustar.fr
wp_replicas: 1
storage_class_name: ebs-sc
storage_size: 10Gi
wp_memory_request: 256Mi
wp_memory_limit: 512Mi
wp_cpu_request: 250m
wp_cpu_limit: 500m
kubeconfig: /etc/rancher/k3s/k3s.yaml

# Default database variables (seront surchargées par les secrets)
db_host: "{{ db_secrets.MYSQL_HOST | default('localhost') }}"
db_name: "{{ db_secrets.MYSQL_DATABASE | default('wordpress') }}"
db_user: "{{ db_secrets.MYSQL_USER | default('wordpress') }}"
db_password: "{{ db_secrets.MYSQL_PASSWORD | default('change_me') }}"

================
File: ansible/roles/wordpress/tasks/ingress.yml
================
---
- name: Get ingress controller service information
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Service
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: ingress_service

- name: Display ingress service details
  debug:
    var: ingress_service

- name: Apply WordPress Ingress
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('template', '../templates/ingress.yaml.j2') | from_yaml }}"
    namespace: "{{ wp_namespace }}"
  register: ingress_result

- name: Wait for Ingress to be ready
  kubernetes.core.k8s_info:
    api_version: networking.k8s.io/v1
    kind: Ingress
    name: wordpress-ingress
    namespace: "{{ wp_namespace }}"
  register: ingress_status
  until: ingress_status.resources[0].status is defined
  retries: 30
  delay: 10

- name: Display Ingress status
  debug:
    var: ingress_status

================
File: ansible/roles/wordpress/tasks/main.yml
================
---
- name: Ensure kubeconfig directory exists
  file:
    path: "{{ ansible_env.HOME }}/.kube"
    state: directory
    mode: '0755'

- name: Copy kubeconfig locally
  copy:
    src: /etc/rancher/k3s/k3s.yaml
    dest: "{{ ansible_env.HOME }}/.kube/config"
    mode: '0600'
    remote_src: yes

- name: Set KUBECONFIG environment variable
  set_fact:
    kubeconfig: "{{ ansible_env.HOME }}/.kube/config"

- name: Update apt cache
  apt:
    update_cache: yes
  become: true

- name: Install required packages
  apt:
    name: 
      - python3-yaml
      - python3-jsonpatch
      - python3-kubernetes
    state: present
  become: true

- name: Create WordPress namespace
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ wp_namespace }}"

- name: Include secrets tasks
  include_tasks: secrets.yml

- name: Delete existing PVC if exists
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: absent
    api_version: v1
    kind: PersistentVolumeClaim
    namespace: "{{ wp_namespace }}"
    name: wp-pvc
  ignore_errors: true

- name: Create WordPress resources
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    template: "{{ item }}"
    namespace: "{{ wp_namespace }}"
  loop:
    - storage.yaml.j2
    - configmap.yaml.j2
    - secret.yaml.j2
    - deployment.yaml.j2
    - service.yaml.j2

- name: Include Ingress tasks
  include_tasks: ingress.yml
  tags:
    - wordpress
    - ingress

- name: Wait for WordPress deployment
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Deployment
    name: wordpress
    namespace: "{{ wp_namespace }}"
  register: wp_deployment
  until: wp_deployment.resources[0].status.availableReplicas is defined and wp_deployment.resources[0].status.availableReplicas > 0
  retries: 30
  delay: 10

- name: Display WordPress deployment status
  debug:
    var: wp_deployment.resources[0].status

================
File: ansible/roles/wordpress/tasks/secrets.yml
================
---
- name: Get AWS secrets
  block:
    - name: Execute get_secret.py
      script: ../common/files/get_secret.py
      register: secret_output
      delegate_to: localhost
      become: false

    - name: Parse JSON output
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true

    - name: Verify secrets loaded
      assert:
        that:
          - db_secrets is defined
          - db_secrets.MYSQL_HOST is defined
          - db_secrets.MYSQL_DATABASE is defined
          - db_secrets.MYSQL_USER is defined
          - db_secrets.MYSQL_PASSWORD is defined
        fail_msg: "Required database secrets are missing"
  rescue:
    - name: Debug AWS secret retrieval
      debug:
        msg: "Failed to get AWS secrets: {{ secret_output.stderr if secret_output is defined else 'Unknown error' }}"
      failed_when: true

================
File: ansible/roles/wordpress/templates/configmap.yaml.j2
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: wordpress-config
data:
  WORDPRESS_DB_HOST: "{{ db_host }}"
  WORDPRESS_DB_NAME: "{{ db_name }}"
  WORDPRESS_DB_USER: "{{ db_user }}"

================
File: ansible/roles/wordpress/templates/deployment.yaml.j2
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  namespace: {{ wp_namespace }}
  labels:
    app: wordpress
spec:
  replicas: {{ wp_replicas }}
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
        - name: wordpress
          image: wordpress:latest
          ports:
            - containerPort: 80
          env:
            - name: WORDPRESS_DB_HOST
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_HOST
            - name: WORDPRESS_DB_USER
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_USER
            - name: WORDPRESS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_PASSWORD
            - name: WORDPRESS_DB_NAME
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_NAME

================
File: ansible/roles/wordpress/templates/ingress.yaml.j2
================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wordpress-ingress
  namespace: {{ wp_namespace }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: "64m"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "32k"
    nginx.ingress.kubernetes.io/proxy-buffers-number: "16"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "300"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "300"
spec:
  ingressClassName: nginx
  rules:
    - host: {{ wp_domain }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: wordpress
                port:
                  number: 80

================
File: ansible/roles/wordpress/templates/secret.yaml.j2
================
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-secret
type: Opaque
data:
  WORDPRESS_DB_PASSWORD: "{{ db_secrets.MYSQL_PASSWORD | b64encode }}"
  WORDPRESS_DB_HOST: "{{ db_secrets.MYSQL_HOST | b64encode }}"
  WORDPRESS_DB_USER: "{{ db_secrets.MYSQL_USER | b64encode }}"
  WORDPRESS_DB_NAME: "{{ db_secrets.MYSQL_DATABASE | b64encode }}"

================
File: ansible/roles/wordpress/templates/service.yaml.j2
================
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  namespace: {{ wp_namespace }}
  labels:
    app: wordpress
spec:
  selector:
    app: wordpress
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

================
File: ansible/roles/wordpress/templates/storage.yaml.j2
================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pvc
  namespace: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: ebs-sc

================
File: ansible/roles/wordpress/templates/wordpress.conf.j2
================
server {
    listen 80;
    server_name {{ domain_name }};
    
    access_log /var/log/nginx/wordpress_access.log combined buffer=512k;
    error_log /var/log/nginx/wordpress_error.log;

    location / {
        proxy_pass http://127.0.0.1:{{ ingress_http_nodeport }};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
        
        proxy_connect_timeout 180s;
        proxy_send_timeout 180s;
        proxy_read_timeout 180s;
        
        client_max_body_size 64M;
    }
}

================
File: ansible/roles/wordpress/vars/main.yml
================
# roles/wordpress/vars/main.yml
---
db_host: "{{ db_secrets.MYSQL_HOST }}"
db_name: "{{ db_secrets.MYSQL_DATABASE }}"
db_user: "{{ db_secrets.MYSQL_USER }}"
db_password: "{{ db_secrets.MYSQL_PASSWORD }}"

================
File: ansible/ansible.cfg
================
[defaults]
inventory = inventory/hosts.yml
private_key_file = ~/.ssh/test-aws-key-pair-new.pem
host_key_checking = False
remote_user = ubuntu
retry_files_enabled = False
gathering = smart
fact_caching = jsonfile
fact_caching_connection = ~/.ansible/facts_cache
log_path = ~/.ansible/ansible.log
roles_path = roles
nocows = True
remote_tmp = /tmp/.ansible-${USER}/tmp


[ssh_connection]
pipelining = True
ssh_args = -o ControlMaster=auto -o ControlPersist=600s -o ServerAliveInterval=30 -o ServerAliveCountMax=10 -o TCPKeepAlive=yes
control_path = %(directory)s/%%h-%%r
control_path_dir = ~/.ansible/cp
retries = 5
timeout = 60

================
File: ansible/repomix-output.txt
================
This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-02-03T15:43:32.905Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
group_vars/
  all.yml
inventory/
  hosts.yml
roles/
  common/
    defaults/
      main.yml
    files/
      get_secret.py
    handlers/
      main.yml
    tasks/
      main.yml
  docker/
    defaults/
      main.yml
    handlers/
      main.yml
    tasks/
      main.yml
  k3s/
    defaults/
      main.yml
    handlers/
      main.yml
    tasks/
      ebs-csi.yml
      helm.yml
      main.yml
    templates/
      ebs-csi-values.yml.j2
  nginx/
    default/
      main.yml
    handlers/
      main.yml
    tasks/
      main.yml
    templates/
      wordpress.conf.j2
  wordpress/
    defaults/
      main.yml
    tasks/
      ingress.yml
      main.yml
      secrets.yml
    templates/
      configmap.yaml.j2
      deployment.yaml.j2
      ingress.yaml.j2
      secret.yaml.j2
      service.yaml.j2
      storage.yaml.j2
      wordpress.conf.j2
    vars/
      main.yml
ansible.cfg
requirements.yml
run.sh
site.yml

================================================================
Files
================================================================

================
File: group_vars/all.yml
================
# ansible/group_vars/all.yml
---
# Variables globales communes à tous les serveurs
timezone: UTC
environment: test
aws_region: eu-west-3
ebs_csi_enabled: true
cloud_provider: aws
k8s_storage_class: ebs-sc

================
File: inventory/hosts.yml
================
all:
  vars:
    ansible_python_interpreter: /usr/bin/python3
    pip_package: python3-pip
    pip_install_packages:
      - name: boto3
  children:
    wordpress:
      hosts:
        wp-test:
          ansible_host: 35.180.222.29
          ansible_user: ubuntu
          ansible_ssh_private_key_file: ~/.ssh/test-aws-key-pair-new.pem

================
File: roles/common/defaults/main.yml
================
# roles/common/defaults/main.yml
---
system_packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - software-properties-common
  - python3-pip
  - python3-boto3
  - python3-botocore
  - nfs-common
  - git
  - vim
timezone: UTC
environment: test

================
File: roles/common/files/get_secret.py
================
#!/usr/bin/env python3
import boto3
import json
import sys

try:
    client = boto3.client('secretsmanager', region_name='eu-west-3')
    response = client.get_secret_value(SecretId='book')
    print(response['SecretString'])
except Exception as e:
    print(f"Erreur: {str(e)}", file=sys.stderr)
    sys.exit(1)

================
File: roles/common/handlers/main.yml
================
# roles/common/handlers/main.yml
---
- name: restart systemd-resolved
  service:
    name: systemd-resolved
    state: restarted

================
File: roles/common/tasks/main.yml
================
---
- name: Wait for cloud-init to complete  
  command: cloud-init status --wait  
  register: cloud_init_result  
  until: cloud_init_result.rc == 0  
  retries: 30  
  delay: 10  
  changed_when: false
  
- name: Reset connection to allow group changes to take effect  
  meta: reset_connection

- name: Wait for apt lock  
  shell: while sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1 || sudo fuser /var/lib/apt/lists/lock >/dev/null 2>&1; do sleep 1; done  
  changed_when: false
  
- name: Clean apt  
  apt:
    clean: yes  
    autoclean: yes  
    autoremove: yes  
    force_apt_get: yes  
  become: true

- name: Force kill apt/dpkg processes  
  shell: |
    killall apt apt-get dpkg 2>/dev/null || true  
    rm -f /var/lib/apt/lists/lock /var/cache/apt/archives/lock /var/lib/dpkg/lock* 2>/dev/null || true  
    dpkg --configure -a  
  changed_when: false  
  ignore_errors: true  
  become: true

- name: Update package cache  
  apt:
    update_cache: yes  
    cache_valid_time: 3600  
    force_apt_get: yes  
  register: apt_update  
  retries: 5  
  delay: 10  
  until: apt_update is success  
  become: true

- name: Install system packages  
  apt:
    name:
      - python3  
      - python3-pip  
      - python3-boto3  
      - python3-botocore  
      - python3-kubernetes  
      - python3-openshift  
      - git  
      - vim  
    state: present  
    force_apt_get: yes  
    update_cache: yes  
  become: true  
  register: pkg_install  
  retries: 3  
  delay: 10  
  until: pkg_install is success

================
File: roles/docker/defaults/main.yml
================
# roles/docker/defaults/main.yml
---
docker_users:
  - "{{ ansible_user }}"

================
File: roles/docker/handlers/main.yml
================
# roles/docker/handlers/main.yml
---
- name: restart docker
  service:
    name: docker
    state: restarted

================
File: roles/docker/tasks/main.yml
================
- name: Install required packages
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg-agent
      - software-properties-common
    state: present

- name: Add Docker GPG key
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: Add Docker repository
  apt_repository:
    repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable
    state: present

- name: Install Docker
  apt:
    name:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - docker-compose-plugin
    state: present
    update_cache: yes

- name: Start and enable Docker
  service:
    name: docker
    state: started
    enabled: yes

- name: Add users to docker group
  user:
    name: "{{ item }}"
    groups: docker
    append: yes
  with_items: "{{ docker_users }}"

- name: Configure Docker daemon
  copy:
    content: |
      {
        "log-driver": "json-file",
        "log-opts": {
          "max-size": "100m",
          "max-file": "3"
        }
      }
    dest: /etc/docker/daemon.json
  notify: restart docker

================
File: roles/k3s/defaults/main.yml
================
# roles/k3s/defaults/main.yml
---
k3s_version: "v1.28.4+k3s2"
k3s_server_args: "--disable traefik --disable servicelb"
ebs_csi_enabled: true
ebs_csi_version: "2.26.0"
storage_class_name: "ebs-sc"
aws_region: "eu-west-3"
wp_namespace: wordpress
kubeconfig: /etc/rancher/k3s/k3s.yaml
security_group_id: "sg-0e2ab25ac27f8bbc5"
project_name: "wordpress-mmustar"
environment: "test"

================
File: roles/k3s/handlers/main.yml
================
# roles/k3s/handlers/main.yml
---
- name: restart k3s
  service:
    name: k3s
    state: restarted

- name: wait for k3s
  wait_for:
    path: "{{ kubeconfig }}"
    delay: 10
    timeout: 300

================
File: roles/k3s/tasks/ebs-csi.yml
================
# roles/k3s/tasks/ebs-csi.yml
---
- name: Fetch AWS Secrets
  command: "aws secretsmanager get-secret-value --secret-id book --region {{ aws_region }} --output json"
  register: secrets_result
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set variables from secrets
  set_fact:
    aws_account_id: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_account_id') }}"
    aws_ebs_csi_role_name: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_ebs_csi_role_name') }}"

- name: Check if EBS CSI Driver is already installed
  command: helm list -n kube-system
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  register: helm_list
  changed_when: false

- name: Delete old StorageClass if exists
  command: kubectl delete storageclass ebs-sc
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  ignore_errors: true
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Add EBS CSI Helm repository
  kubernetes.core.helm_repository:
    name: aws-ebs-csi-driver
    repo_url: https://kubernetes-sigs.github.io/aws-ebs-csi-driver
    state: present
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Update Helm repositories
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

- name: Template EBS CSI values
  template:
    src: ebs-csi-values.yml.j2
    dest: /tmp/ebs-csi-values.yml
    mode: '0644'
  when: ebs_csi_enabled

- name: Create kube-system namespace if not exists
  kubernetes.core.k8s:
    name: kube-system
    api_version: v1
    kind: Namespace
    state: present
    kubeconfig: /etc/rancher/k3s/k3s.yaml

- name: Install AWS EBS CSI Driver
  kubernetes.core.helm:
    name: aws-ebs-csi-driver
    chart_ref: aws-ebs-csi-driver/aws-ebs-csi-driver
    release_namespace: kube-system
    create_namespace: true
    values_files:
      - /tmp/ebs-csi-values.yml
    chart_version: "{{ ebs_csi_version }}"
    wait: true
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Wait for EBS CSI Driver pods
  shell: >
    kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get pods -n kube-system 
    -l app.kubernetes.io/name=aws-ebs-csi-driver --no-headers
  register: ebs_csi_pods
  until: ebs_csi_pods.rc == 0 and ebs_csi_pods.stdout_lines | length > 0 and ebs_csi_pods.stdout.find('Running') != -1
  retries: 30
  delay: 10
  when: ebs_csi_enabled

- name: Clean up temporary values file
  file:
    path: /tmp/ebs-csi-values.yml
    state: absent
  when: ebs_csi_enabled

================
File: roles/k3s/tasks/helm.yml
================
# roles/k3s/tasks/helm.yml
---
- name: Add Helm apt key
  apt_key:
    url: https://baltocdn.com/helm/signing.asc
    state: present
  become: true

- name: Add Helm repository
  apt_repository:
    repo: deb https://baltocdn.com/helm/stable/debian/ all main
    state: present
    filename: helm
  become: true

- name: Install Helm
  apt:
    name: helm
    state: present
    update_cache: yes
  become: true
  register: helm_install
  retries: 3
  delay: 5
  until: helm_install is success

- name: Verify Helm installation
  command: helm version
  register: helm_version
  changed_when: false
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml

- name: Add stable Helm repository
  command: helm repo add stable https://charts.helm.sh/stable
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true
  failed_when: false

- name: Update Helm repos
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

================
File: roles/k3s/tasks/main.yml
================
# roles/k3s/tasks/main.yml
---
# Étape 1: Configuration initiale
- name: Get subnet IDs from AWS
  shell: >
    aws ec2 describe-subnets
    --region {{ aws_region }}
    --filters "Name=tag:Project,Values={{ project_name }}" "Name=tag:Environment,Values={{ environment }}"
    --query "Subnets[0].SubnetId"
    --output text
  register: subnet_info
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set subnet ID
  set_fact:
    public_subnet_id: "{{ subnet_info.stdout }}"
  when: subnet_info.stdout != ""

# Étape 2: Installation de K3s
- name: Download k3s installation script
  get_url:
    url: https://get.k3s.io
    dest: /tmp/k3s-install.sh
    mode: '0755'

- name: Install k3s
  shell: 
    cmd: INSTALL_K3S_VERSION={{ k3s_version }} /tmp/k3s-install.sh --write-kubeconfig-mode 644 {{ k3s_server_args }}
  args:
    creates: /usr/local/bin/k3s
  environment:
    INSTALL_K3S_EXEC: "{{ k3s_server_args }}"
  register: k3s_install

- name: Wait for k3s service to start
  systemd:
    name: k3s
    state: started
    enabled: yes
  register: k3s_service

- name: Configure kubeconfig permissions
  file:
    path: /etc/rancher/k3s/k3s.yaml
    mode: '0644'
  become: true

- name: Set KUBECONFIG environment variable
  lineinfile:
    path: /home/{{ ansible_user }}/.bashrc
    line: "export KUBECONFIG=/etc/rancher/k3s/k3s.yaml"
    state: present
  become: true

# Étape 3: Vérification de l'installation
- name: Wait for k3s node to be ready
  shell: kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes
  register: node_status
  until: "'Ready' in node_status.stdout"
  retries: 30
  delay: 10
  become: true

# Étape 4: Configuration du Security Group
- name: Add NodePort range to security group
  shell: >
    aws ec2 authorize-security-group-ingress 
    --region {{ aws_region }}
    --group-id {{ security_group_id }}
    --protocol tcp 
    --port 30000-32767 
    --cidr 0.0.0.0/0
  delegate_to: localhost
  become: false
  register: sg_result
  failed_when: 
    - sg_result.rc != 0 
    - '"InvalidPermission.Duplicate" not in sg_result.stderr'
  changed_when: sg_result.rc == 0

# Étape 5: Installation de Helm
- name: Include Helm installation tasks
  include_tasks: helm.yml

# Étape 6: Installation de NGINX Ingress
- name: Add ingress-nginx repository
  command: helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true
  ignore_errors: true

- name: Update Helm repos
  command: helm repo update
  environment:
    KUBECONFIG: /etc/rancher/k3s/k3s.yaml
  changed_when: true

- name: Install ingress-nginx with Helm
  kubernetes.core.helm:
    name: ingress-nginx
    chart_ref: ingress-nginx/ingress-nginx
    release_namespace: ingress-nginx
    create_namespace: true
    release_values:
      controller:
        service:
          type: NodePort
        kind: DaemonSet
    kubeconfig: /etc/rancher/k3s/k3s.yaml
  register: nginx_helm_result

- name: Wait for ingress-nginx pods to be Running
  shell: >
    kubectl get pods -n ingress-nginx -l app.kubernetes.io/component=controller --no-headers
  register: nginx_pod_status
  until: nginx_pod_status.rc == 0 and 'Running' in nginx_pod_status.stdout
  retries: 30
  delay: 10

- name: Display ingress-nginx pod logs if not running
  shell: kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=50
  register: nginx_logs
  when: "'Running' not in nginx_pod_status.stdout"

- name: Show pod logs
  debug:
    var: nginx_logs.stdout_lines
  when: nginx_logs is defined

# Étape 7: Installation du EBS CSI Driver
- name: Include EBS CSI tasks
  include_tasks: ebs-csi.yml
  when: ebs_csi_enabled

# Étape 8: Vérification finale
- name: Verify cluster status
  command: kubectl --kubeconfig=/etc/rancher/k3s/k3s.yaml get nodes,pods --all-namespaces
  register: cluster_status
  changed_when: false

- name: Show cluster status
  debug:
    var: cluster_status.stdout_lines

================
File: roles/k3s/templates/ebs-csi-values.yml.j2
================
# roles/k3s/templates/ebs-csi-values.yml.j2
controller:
  region: "{{ aws_region }}"
  serviceAccount:
    create: true
    name: ebs-csi-controller-sa
    annotations:
      eks.amazonaws.com/role-arn: null
      
node:
  serviceAccount:
    create: true
    name: ebs-csi-node-sa

storageClasses:
- name: ebs-sc
  provisioner: ebs.csi.aws.com
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  volumeBindingMode: WaitForFirstConsumer
  allowVolumeExpansion: true
  reclaimPolicy: Delete
  parameters:
    type: gp3

================
File: roles/nginx/default/main.yml
================
domain_name: "test.mmustar.fr"
cert_email: "mmustar@free.fr"
ingress_http_nodeport: 30635

================
File: roles/nginx/handlers/main.yml
================
---
- name: Reload Nginx
  service:
    name: nginx
    state: reloaded
  become: true

- name: Restart Nginx
  service:
    name: nginx
    state: restarted
  become: true

- name: Enable Nginx
  service:
    name: nginx
    enabled: true
  become: true

- name: Test Nginx configuration
  command: nginx -t
  become: true
  register: nginx_config_test
  changed_when: false

================
File: roles/nginx/tasks/main.yml
================
---
# Installation de Nginx
- name: Install Nginx
  apt:
    name: nginx
    state: present
    update_cache: yes
  become: true

# Configuration Nginx pour WordPress
- name: Create wordpress.conf
  template:
    src: wordpress.conf.j2
    dest: /etc/nginx/sites-available/wordpress.conf
    mode: '0644'
  become: true
  notify: Reload Nginx

- name: Enable wordpress site
  file:
    src: /etc/nginx/sites-available/wordpress.conf
    dest: /etc/nginx/sites-enabled/wordpress.conf
    state: link
  become: true
  notify: Reload Nginx

- name: Remove default nginx site
  file:
    path: /etc/nginx/sites-enabled/default
    state: absent
  become: true
  notify: Reload Nginx

- name: Ensure Nginx is started and enabled
  service:
    name: nginx
    state: started
    enabled: yes
  become: true

================
File: roles/nginx/templates/wordpress.conf.j2
================
server {
    listen 80;
    server_name {{ domain_name }};

    location / {
        proxy_pass http://127.0.0.1:{{ ingress_http_nodeport }};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
        
        client_max_body_size 64M;
    }
}

================
File: roles/wordpress/defaults/main.yml
================
# roles/wordpress/defaults/main.yml
---
wp_namespace: wordpress
wp_domain: test.mmustar.fr
wp_replicas: 1
storage_class_name: ebs-sc
storage_size: 10Gi
wp_memory_request: 256Mi
wp_memory_limit: 512Mi
wp_cpu_request: 250m
wp_cpu_limit: 500m
kubeconfig: /etc/rancher/k3s/k3s.yaml

# Default database variables (seront surchargées par les secrets)
db_host: "{{ db_secrets.MYSQL_HOST | default('localhost') }}"
db_name: "{{ db_secrets.MYSQL_DATABASE | default('wordpress') }}"
db_user: "{{ db_secrets.MYSQL_USER | default('wordpress') }}"
db_password: "{{ db_secrets.MYSQL_PASSWORD | default('change_me') }}"

================
File: roles/wordpress/tasks/ingress.yml
================
---
- name: Get ingress controller service information
  kubernetes.core.k8s_info:
    api_version: v1
    kind: Service
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: ingress_service

- name: Display ingress service details
  debug:
    var: ingress_service

- name: Apply WordPress Ingress
  kubernetes.core.k8s:
    state: present
    definition: "{{ lookup('template', '../templates/ingress.yaml.j2') | from_yaml }}"
    namespace: "{{ wp_namespace }}"
  register: ingress_result

- name: Wait for Ingress to be ready
  kubernetes.core.k8s_info:
    api_version: networking.k8s.io/v1
    kind: Ingress
    name: wordpress-ingress
    namespace: "{{ wp_namespace }}"
  register: ingress_status
  until: ingress_status.resources[0].status is defined
  retries: 30
  delay: 10

- name: Display Ingress status
  debug:
    var: ingress_status

================
File: roles/wordpress/tasks/main.yml
================
---
- name: Ensure kubeconfig directory exists
  file:
    path: "{{ ansible_env.HOME }}/.kube"
    state: directory
    mode: '0755'

- name: Copy kubeconfig locally
  copy:
    src: /etc/rancher/k3s/k3s.yaml
    dest: "{{ ansible_env.HOME }}/.kube/config"
    mode: '0600'
    remote_src: yes

- name: Set KUBECONFIG environment variable
  set_fact:
    kubeconfig: "{{ ansible_env.HOME }}/.kube/config"

- name: Update apt cache
  apt:
    update_cache: yes
  become: true

- name: Install required packages
  apt:
    name: 
      - python3-yaml
      - python3-jsonpatch
      - python3-kubernetes
    state: present
  become: true

- name: Create WordPress namespace
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ wp_namespace }}"

- name: Include secrets tasks
  include_tasks: secrets.yml

- name: Delete existing PVC if exists
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: absent
    api_version: v1
    kind: PersistentVolumeClaim
    namespace: "{{ wp_namespace }}"
    name: wp-pvc
  ignore_errors: true

- name: Create WordPress resources
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    template: "{{ item }}"
    namespace: "{{ wp_namespace }}"
  loop:
    - storage.yaml.j2
    - configmap.yaml.j2
    - secret.yaml.j2
    - deployment.yaml.j2
    - service.yaml.j2

- name: Include Ingress tasks
  include_tasks: ingress.yml
  tags:
    - wordpress
    - ingress

- name: Wait for WordPress deployment
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Deployment
    name: wordpress
    namespace: "{{ wp_namespace }}"
  register: wp_deployment
  until: wp_deployment.resources[0].status.availableReplicas is defined and wp_deployment.resources[0].status.availableReplicas > 0
  retries: 30
  delay: 10

- name: Display WordPress deployment status
  debug:
    var: wp_deployment.resources[0].status

================
File: roles/wordpress/tasks/secrets.yml
================
---
- name: Get AWS secrets
  block:
    - name: Execute get_secret.py
      script: ../common/files/get_secret.py
      register: secret_output
      delegate_to: localhost
      become: false

    - name: Parse JSON output
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true

    - name: Verify secrets loaded
      assert:
        that:
          - db_secrets is defined
          - db_secrets.MYSQL_HOST is defined
          - db_secrets.MYSQL_DATABASE is defined
          - db_secrets.MYSQL_USER is defined
          - db_secrets.MYSQL_PASSWORD is defined
        fail_msg: "Required database secrets are missing"
  rescue:
    - name: Debug AWS secret retrieval
      debug:
        msg: "Failed to get AWS secrets: {{ secret_output.stderr if secret_output is defined else 'Unknown error' }}"
      failed_when: true

================
File: roles/wordpress/templates/configmap.yaml.j2
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: wordpress-config
data:
  WORDPRESS_DB_HOST: "{{ db_host }}"
  WORDPRESS_DB_NAME: "{{ db_name }}"
  WORDPRESS_DB_USER: "{{ db_user }}"

================
File: roles/wordpress/templates/deployment.yaml.j2
================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  namespace: {{ wp_namespace }}
  labels:
    app: wordpress
spec:
  replicas: {{ wp_replicas }}
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
        - name: wordpress
          image: wordpress:latest
          ports:
            - containerPort: 80
          env:
            - name: WORDPRESS_DB_HOST
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_HOST
            - name: WORDPRESS_DB_USER
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_USER
            - name: WORDPRESS_DB_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_PASSWORD
            - name: WORDPRESS_DB_NAME
              valueFrom:
                secretKeyRef:
                  name: wordpress-secret
                  key: WORDPRESS_DB_NAME

================
File: roles/wordpress/templates/ingress.yaml.j2
================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wordpress-ingress
  namespace: {{ wp_namespace }}
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/proxy-body-size: "64m"
    nginx.ingress.kubernetes.io/proxy-connect-timeout: "180"
    nginx.ingress.kubernetes.io/proxy-send-timeout: "180"
    nginx.ingress.kubernetes.io/proxy-read-timeout: "180"
    nginx.ingress.kubernetes.io/proxy-buffer-size: "128k"
spec:
  ingressClassName: nginx
  rules:
    - host: {{ wp_domain }}
      http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: wordpress
                port:
                  number: 80

================
File: roles/wordpress/templates/secret.yaml.j2
================
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-secret
type: Opaque
data:
  WORDPRESS_DB_PASSWORD: "{{ db_secrets.MYSQL_PASSWORD | b64encode }}"
  WORDPRESS_DB_HOST: "{{ db_secrets.MYSQL_HOST | b64encode }}"
  WORDPRESS_DB_USER: "{{ db_secrets.MYSQL_USER | b64encode }}"
  WORDPRESS_DB_NAME: "{{ db_secrets.MYSQL_DATABASE | b64encode }}"

================
File: roles/wordpress/templates/service.yaml.j2
================
apiVersion: v1
kind: Service
metadata:
  name: wordpress
  namespace: {{ wp_namespace }}
  labels:
    app: wordpress
spec:
  selector:
    app: wordpress
  ports:
    - protocol: TCP
      port: 80
      targetPort: 80
  type: ClusterIP

================
File: roles/wordpress/templates/storage.yaml.j2
================
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pvc
  namespace: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: ebs-sc

================
File: roles/wordpress/templates/wordpress.conf.j2
================
server {
    listen 80;
    server_name {{ domain_name }};
    
    access_log /var/log/nginx/wordpress_access.log combined buffer=512k;
    error_log /var/log/nginx/wordpress_error.log;

    location / {
        proxy_pass http://127.0.0.1:{{ ingress_http_nodeport }};
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
        
        proxy_connect_timeout 180s;
        proxy_send_timeout 180s;
        proxy_read_timeout 180s;
        
        client_max_body_size 64M;
    }
}

================
File: roles/wordpress/vars/main.yml
================
# roles/wordpress/vars/main.yml
---
db_host: "{{ db_secrets.MYSQL_HOST }}"
db_name: "{{ db_secrets.MYSQL_DATABASE }}"
db_user: "{{ db_secrets.MYSQL_USER }}"
db_password: "{{ db_secrets.MYSQL_PASSWORD }}"

================
File: ansible.cfg
================
[defaults]
inventory = inventory/hosts.yml
private_key_file = ~/.ssh/test-aws-key-pair-new.pem
host_key_checking = False
remote_user = ubuntu
retry_files_enabled = False
gathering = smart
fact_caching = jsonfile
fact_caching_connection = ~/.ansible/facts_cache
log_path = ~/.ansible/ansible.log
roles_path = roles
nocows = True
remote_tmp = /tmp/.ansible-${USER}/tmp


[ssh_connection]
pipelining = True
ssh_args = -o ControlMaster=auto -o ControlPersist=600s -o ServerAliveInterval=30 -o ServerAliveCountMax=10 -o TCPKeepAlive=yes
control_path = %(directory)s/%%h-%%r
control_path_dir = ~/.ansible/cp
retries = 5
timeout = 60

================
File: requirements.yml
================
---
collections:
  - name: kubernetes.core
    version: "2.4.0"
  - name: community.general
    version: "7.5.0"

roles: []

================
File: run.sh
================
# run.sh
#!/bin/bash
set -euo pipefail

# Vérification des variables d'environnement requises
if [[ -z "${AWS_ACCESS_KEY_ID:-}" ]] || [[ -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
    echo "Erreur: Les credentials AWS doivent être définis"
    exit 1
fi

# Vérification de la présence des fichiers nécessaires
required_files=("site.yml" "ansible.cfg" "inventory/hosts.yml")
for file in "${required_files[@]}"; do
    if [[ ! -f "$file" ]]; then
        echo "Erreur: Fichier requis manquant: $file"
        exit 1
    fi
done

# Vérification de la syntaxe
echo "Vérification de la syntaxe du playbook..."
if ! ansible-playbook --syntax-check site.yml; then
    echo "Erreur: Le playbook contient des erreurs de syntaxe"
    exit 1
fi

# Exécution du playbook
echo "Exécution du playbook..."
ansible-playbook site.yml -v

# Vérification du statut de sortie
if [[ $? -eq 0 ]]; then
    echo "Le playbook s'est exécuté avec succès"
else
    echo "Erreur lors de l'exécution du playbook"
    exit 1
fi

================
File: site.yml
================
- name: Configuration complète des serveurs WordPress
  hosts: wordpress
  become: true
  vars:
    aws_region: eu-west-3
    ebs_csi_enabled: true
    nginx_server_name: test.mmustar.fr
    domain_name: test.mmustar.fr
    wp_namespace: wordpress
    kubeconfig: /etc/rancher/k3s/k3s.yaml

  pre_tasks:
    - name: Installer net-tools
      apt:
        name: net-tools
        state: present
      tags:
        - net-tools

    - name: Get AWS secret
      block:
        - script: roles/common/files/get_secret.py
          register: secret_output
          delegate_to: localhost
          become: false
          changed_when: false
      rescue:
        - fail:
            msg: "Impossible de récupérer les secrets AWS. Vérifiez vos credentials AWS et l'existence du secret."

    - name: Set database secrets
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true
      when: secret_output is defined

    - name: Debug db_secrets
      debug:
        msg: "DB Secrets loaded: {{ db_secrets is defined }}"
      when: secret_output is defined

  roles:
    - { role: common, tags: ['common'] }
    - { role: docker, tags: ['docker'] }
    - { role: k3s, tags: ['k3s'] }
    - { role: wordpress, tags: ['wordpress'] }
    - { role: nginx, tags: ['nginx'] }

================
File: ansible/requirements.yml
================
---
collections:
  - name: kubernetes.core
    version: "2.4.0"
  - name: community.general
    version: "7.5.0"

roles: []

================
File: ansible/run.sh
================
# run.sh
#!/bin/bash
set -euo pipefail

# Vérification des variables d'environnement requises
if [[ -z "${AWS_ACCESS_KEY_ID:-}" ]] || [[ -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
    echo "Erreur: Les credentials AWS doivent être définis"
    exit 1
fi

# Vérification de la présence des fichiers nécessaires
required_files=("site.yml" "ansible.cfg" "inventory/hosts.yml")
for file in "${required_files[@]}"; do
    if [[ ! -f "$file" ]]; then
        echo "Erreur: Fichier requis manquant: $file"
        exit 1
    fi
done

# Vérification de la syntaxe
echo "Vérification de la syntaxe du playbook..."
if ! ansible-playbook --syntax-check site.yml; then
    echo "Erreur: Le playbook contient des erreurs de syntaxe"
    exit 1
fi

# Exécution du playbook
echo "Exécution du playbook..."
ansible-playbook site.yml -v

# Vérification du statut de sortie
if [[ $? -eq 0 ]]; then
    echo "Le playbook s'est exécuté avec succès"
else
    echo "Erreur lors de l'exécution du playbook"
    exit 1
fi

================
File: ansible/site.yml
================
- name: Configuration complète des serveurs WordPress
  hosts: wordpress
  become: true
  vars:
    aws_region: eu-west-3
    ebs_csi_enabled: true
    nginx_server_name: test.mmustar.fr
    domain_name: test.mmustar.fr
    wp_namespace: wordpress
    kubeconfig: /etc/rancher/k3s/k3s.yaml

  pre_tasks:
    - name: Installer net-tools
      apt:
        name: net-tools
        state: present
      tags:
        - net-tools

    - name: Get AWS secret
      block:
        - script: roles/common/files/get_secret.py
          register: secret_output
          delegate_to: localhost
          become: false
          changed_when: false
      rescue:
        - fail:
            msg: "Impossible de récupérer les secrets AWS. Vérifiez vos credentials AWS et l'existence du secret."

    - name: Set database secrets
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true
      when: secret_output is defined

    - name: Debug db_secrets
      debug:
        msg: "DB Secrets loaded: {{ db_secrets is defined }}"
      when: secret_output is defined

  roles:
    - { role: common, tags: ['common'] }
    - { role: docker, tags: ['docker'] }
    - { role: k3s, tags: ['k3s'] }
    - { role: wordpress, tags: ['wordpress'] }
    - { role: nginx, tags: ['nginx'] }

================
File: environments/backend-config/.terraform.lock.hcl
================
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.0.0"
  constraints = "~> 5.0"
  hashes = [
    "h1:swP2uqDPi7bRLe+J4oUGEp8ZPTG4NaAV9QK+Iqgo2ro=",
  ]
}

================
File: environments/backend-config/main.tf
================
# environments/backend-config/main.tf

# Un seul bucket S3 pour tous les environnements
resource "aws_s3_bucket" "terraform_state" {
  bucket = "${var.project_name}-terraform-state"

  lifecycle {
    prevent_destroy = true
  }

  tags = {
    Name    = "${var.project_name}-terraform-state"
    Project = var.project_name
  }
}

# Activation du versioning
resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

# Activation du chiffrement
resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

# Blocage de l'accès public
resource "aws_s3_bucket_public_access_block" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Une seule table DynamoDB pour le verrouillage
resource "aws_dynamodb_table" "terraform_locks" {
  name         = "${var.project_name}-terraform-locks"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  tags = {
    Name    = "${var.project_name}-terraform-locks"
    Project = var.project_name
  }
}

================
File: environments/backend-config/outputs.tf
================
# environments/backend-config/outputs.tf
output "bucket_name" {
  description = "Nom du bucket S3 pour le state Terraform"
  value       = aws_s3_bucket.terraform_state.id
}

output "dynamodb_table_name" {
  description = "Nom de la table DynamoDB pour le verrouillage"
  value       = aws_dynamodb_table.terraform_locks.name
}

================
File: environments/backend-config/variables.tf
================
# environments/backend-config/variables.tf
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "eu-west-3"
}

variable "project_name" {
  description = "Nom du projet"
  type        = string
  default     = "wordpress-mmustar"
}

================
File: environments/backend-config/versions.tf
================
# environments/backend-config/versions.tf
terraform {
  required_version = ">= 1.0.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = var.project_name
      ManagedBy   = "terraform"
    }
  }
}

================
File: environments/modules/compute/main.tf
================
# environments/modules/compute/main.tf
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "wordpress" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = var.instance_type
  subnet_id     = var.subnet_id
  key_name      = var.key_name

  root_block_device {
    volume_size = 20
    volume_type = "gp3"
  }

  tags = {
    Name        = "${var.project_name}-instance-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    prevent_destroy = true
  }
}

resource "aws_eip" "wordpress" {
  count    = var.environment == "test" ? 1 : 0
  instance = aws_instance.wordpress.id
  domain   = "vpc"

  tags = {
    Name        = "${var.project_name}-eip-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/compute/outputs.tf
================
# environments/modules/compute/outputs.tf
output "instance_id" {
  description = "ID of the created EC2 instance"
  value       = aws_instance.wordpress.id
}

output "instance_public_ip" {
  description = "Public IP of the EC2 instance"
  value       = var.environment == "test" ? aws_eip.wordpress[0].public_ip : aws_instance.wordpress.public_ip
}

output "instance_private_ip" {
  description = "Private IP of the EC2 instance"
  value       = aws_instance.wordpress.private_ip
}

================
File: environments/modules/compute/variables.tf
================
# environments/modules/compute/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the instances will be created"
  type        = string
}

variable "subnet_id" {
  description = "Subnet ID where the instances will be created"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t2.micro"
}

variable "key_name" {
  description = "Name of the SSH key pair"
  type        = string
}

================
File: environments/modules/k3s/iam.tf
================
# modules/k3s/iam.tf
resource "aws_iam_openid_connect_provider" "k3s" {
  url             = "https://token.actions.githubusercontent.com"
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = ["1b511abead59c6ce207077c0bf0e0043b1382612"]
}

resource "aws_iam_role" "ebs_csi" {
  name = "${var.environment}-ebs-csi"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = aws_iam_openid_connect_provider.k3s.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${aws_iam_openid_connect_provider.k3s.url}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
      }
    }]
  })
}

resource "aws_iam_role_policy" "ebs_csi" {
  name = "${var.environment}-ebs-csi-policy"
  role = aws_iam_role.ebs_csi.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ec2:CreateSnapshot",
          "ec2:AttachVolume",
          "ec2:DetachVolume",
          "ec2:ModifyVolume",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeInstances",
          "ec2:DescribeSnapshots",
          "ec2:DescribeTags",
          "ec2:DescribeVolumes",
          "ec2:DescribeVolumesModifications",
          "ec2:CreateVolume",
          "ec2:DeleteVolume",
          "ec2:CreateTags",
          "ec2:DeleteTags"
        ]
        Resource = "*"
      }
    ]
  })
}

variable "environment" {
  description = "Environment name"
  type        = string
}

output "ebs_csi_role_arn" {
  value = aws_iam_role.ebs_csi.arn
}

================
File: environments/modules/network/main.tf
================
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "${var.project_name}-vpc-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name        = "${var.project_name}-igw-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_subnet" "public" {
  count                  = length(var.public_subnet_cidrs)
  vpc_id                 = aws_vpc.main.id
  cidr_block             = var.public_subnet_cidrs[count.index]
  availability_zone      = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name        = "${var.project_name}-public-subnet-${count.index + 1}-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name        = "${var.project_name}-public-rt-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_route_table_association" "public" {
  count = length(var.public_subnet_cidrs)

  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

data "aws_availability_zones" "available" {
  state = "available"
}

================
File: environments/modules/network/outputs.tf
================
# environments/modules/network/outputs.tf
output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "vpc_id" {
  value = aws_vpc.main.id
}

output "route_table_id" {
  value = aws_route_table.public.id
}

================
File: environments/modules/network/security.tf
================
# environments/modules/network/security.tf
resource "aws_security_group" "wordpress" {
  name_prefix = "${var.project_name}-sg-${var.environment}"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]  # À restreindre selon vos besoins
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "${var.project_name}-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/network/variables.tf
================
# environments/modules/network/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
  default     = "wordpress-mmustar"
}

# environments/modules/network/variables.tf
variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
  default     = "172.16.0.0/16"  # Modifié pour éviter le conflit avec le RDS VPC
}

variable "public_subnet_cidrs" {
  description = "CIDR blocks for public subnets"
  type        = list(string)
  default     = ["172.16.1.0/24", "172.16.2.0/24"]  # Ajusté en conséquence
}

================
File: environments/modules/security/main.tf
================
# environments/modules/security/main.tf
resource "aws_security_group" "wordpress" {
  name_prefix = "${var.project_name}-wp-${var.environment}"
  description = "Security group for WordPress instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTP"
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTPS"
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # À restreindre selon vos besoins
    description = "Allow SSH"
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow all outbound traffic"
  }

  tags = {
    Name        = "${var.project_name}-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_security_group" "rds" {
  name_prefix = "${var.project_name}-rds-${var.environment}"
  description = "Security group for RDS instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = "tcp"
    security_groups = [aws_security_group.wordpress.id]
    description     = "Allow MySQL from WordPress SG"
  }

  tags = {
    Name        = "${var.project_name}-rds-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/security/outputs.tf
================
# environments/modules/security/outputs.tf
output "wordpress_sg_id" {
  description = "ID of WordPress security group"
  value       = aws_security_group.wordpress.id
}

output "rds_sg_id" {
  description = "ID of RDS security group"
  value       = aws_security_group.rds.id
}

================
File: environments/modules/security/variables.tf
================
# environments/modules/security/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the security groups will be created"
  type        = string
}

================
File: environments/prod/backend.tf
================
# environments/prod/backend.tf
terraform {
  backend "s3" {
    bucket         = "wordpress-mmustar-terraform-state"
    key            = "environments/prod/terraform.tfstate"
    region         = "eu-west-3"
    dynamodb_table = "wordpress-mmustar-terraform-locks"
    encrypt        = true
  }
}

================
File: environments/test/.terraform.lock.hcl
================
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.0.0"
  constraints = "~> 5.0"
  hashes = [
    "h1:swP2uqDPi7bRLe+J4oUGEp8ZPTG4NaAV9QK+Iqgo2ro=",
  ]
}

================
File: environments/test/backend.tf
================
# environments/test/backend.tf
terraform {
  backend "s3" {
    bucket         = "wordpress-mmustar-terraform-state"
    key            = "test/terraform.tfstate"
    region         = "eu-west-3"
    dynamodb_table = "wordpress-mmustar-terraform-locks"
    encrypt        = true
  }
}

================
File: environments/test/main.tf
================
# Provider configuration
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  required_version = ">= 1.0.0"
}

provider "aws" {
  region = "eu-west-3"
}

# Variables
variable "project_name" {
  description = "The name of the project"
  type        = string
}

variable "environment" {
  description = "The environment for deployment"
  type        = string
}

# Define the VPC
resource "aws_vpc" "main" {
  cidr_block = "10.0.0.0/16"

  tags = {
    Name = "main-vpc"
  }
}

# Define the availability zones data source
data "aws_availability_zones" "available" {
  state = "available"
}

# Modules
module "network" {
  source              = "../modules/network"
  environment         = var.environment
  project_name        = var.project_name
  vpc_cidr            = "10.0.0.0/16"
  public_subnet_cidrs = ["10.0.1.0/24", "10.0.2.0/24"]
}

module "k3s" {
  source      = "../modules/k3s"
  environment = var.environment
}

# Security Group
resource "aws_security_group" "wordpress_test" {
  name_prefix = "WP-SecurityGroup-Test-"
  description = "Security group for WordPress test instance"
  vpc_id      = module.network.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "WP-SG-Test"
    Environment = var.environment
    Project     = var.project_name
  }

  depends_on = [module.network]
}

# Existing EIP
data "aws_eip" "wordpress_test" {
  id = "eipalloc-0933b219497dd6c15"
}

# Data source pour le rôle IAM existant
data "aws_iam_role" "ec2_secrets_manager_role" {
  name = "EC2SecretsManagerRole"
}

# IAM Instance Profile pour EC2
resource "aws_iam_instance_profile" "ec2_secrets_manager_profile" {
  name = "ec2-secrets-manager-profile"
  role = data.aws_iam_role.ec2_secrets_manager_role.name
}

# EC2 Instance
resource "aws_instance" "wordpress_test" {
  ami                         = "ami-06e02ae7bdac6b938"
  instance_type               = "t3.medium"
  subnet_id                   = module.network.public_subnet_ids[0]
  vpc_security_group_ids      = [aws_security_group.wordpress_test.id]
  key_name                    = "test-aws-key-pair-new"
  associate_public_ip_address = true
  iam_instance_profile        = aws_iam_instance_profile.ec2_secrets_manager_profile.name

  root_block_device {
    volume_size           = 8
    volume_type          = "gp3"
    delete_on_termination = true
  }

  tags = {
    Name        = "WP-Instance-Test"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    create_before_destroy = true
  }

  depends_on = [
    module.network,
    aws_security_group.wordpress_test,
    aws_iam_instance_profile.ec2_secrets_manager_profile
  ]
}

# EIP Association
resource "aws_eip_association" "wordpress_test" {
  instance_id   = aws_instance.wordpress_test.id
  allocation_id = data.aws_eip.wordpress_test.id
  depends_on    = [aws_instance.wordpress_test]
}

# RDS Security Group and Instance data sources
data "aws_security_group" "rds" {
  id = "sg-00efe258e85b22a30"
}

# Define the RDS instance data source
data "aws_db_instance" "wordpress" {
  db_instance_identifier = "wordpress-db"
}

# Outputs
output "rds_endpoint" {
  value = data.aws_db_instance.wordpress.endpoint
}

output "instance_public_ip" {
  value = aws_instance.wordpress_test.public_ip
}

output "instance_id" {
  value = aws_instance.wordpress_test.id
}

output "eip_public_ip" {
  value = data.aws_eip.wordpress_test.public_ip
}

output "ebs_csi_role_arn" {
  value = module.k3s.ebs_csi_role_arn
}

output "public_subnet_ids" {
  value = module.network.public_subnet_ids
}

================
File: environments/test/variables.tf
================
# environments/test/variables.tf
variable "aws_region" {
  description = "AWS Region"
  type        = string
  default     = "eu-west-3"
}

# SSH key pour instance EC2
variable "key_name" {
  description = "SSH key pair name"
  type        = string
  default     = "test-aws-key-pair-new"  # Remplacez par votre nom de clé
  
}

================
File: scripts/ssh-update.sh
================
#!/bin/bash

# Configuration par défaut
SSH_KEY_PATH="/home/gnou/.ssh/test-aws-key-pair-new.pem"
EC2_USER="ubuntu"
KNOWN_HOSTS="/home/gnou/.ssh/known_hosts"

# Fonction d'aide
show_usage() {
    echo "Usage: $0 [OPTIONS] EC2_IP_ADDRESS"
    echo "Options:"
    echo "  -k, --key PATH    Chemin vers la clé SSH (default: $SSH_KEY_PATH)"
    echo "  -u, --user USER   Utilisateur EC2 (default: $EC2_USER)"
    echo "  -h, --help        Affiche cette aide"
    exit 1
}

# Traitement des arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -k|--key)
            SSH_KEY_PATH="$2"
            shift 2
            ;;
        -u|--user)
            EC2_USER="$2"
            shift 2
            ;;
        -h|--help)
            show_usage
            ;;
        *)
            EC2_IP="$1"
            shift
            ;;
    esac
done

# Vérification des paramètres requis
if [ -z "$EC2_IP" ]; then
    echo "Erreur: Adresse IP de l'instance EC2 manquante"
    show_usage
fi

if [ ! -f "$SSH_KEY_PATH" ]; then
    echo "Erreur: Clé SSH non trouvée: $SSH_KEY_PATH"
    exit 1
fi

# Fonction pour vérifier si l'instance est accessible
check_instance() {
    timeout 5 nc -zv $EC2_IP 22 &>/dev/null
    return $?
}

# Attendre que l'instance soit accessible
echo "Vérification de l'accessibilité de l'instance..."
ATTEMPTS=0
MAX_ATTEMPTS=10

while ! check_instance; do
    ATTEMPTS=$((ATTEMPTS + 1))
    if [ $ATTEMPTS -ge $MAX_ATTEMPTS ]; then
        echo "Erreur: Impossible de se connecter à l'instance après $MAX_ATTEMPTS tentatives"
        exit 1
    fi
    echo "Instance non accessible, nouvelle tentative dans 5 secondes... ($ATTEMPTS/$MAX_ATTEMPTS)"
    sleep 5
done

# Supprimer l'ancienne clé host si elle existe
if ssh-keygen -F $EC2_IP >/dev/null 2>&1; then
    echo "Suppression de l'ancienne clé host..."
    ssh-keygen -f "$KNOWN_HOSTS" -R "$EC2_IP" >/dev/null 2>&1
fi

# Tentative de connexion
echo "Connexion à l'instance $EC2_IP..."
ssh -o StrictHostKeyChecking=accept-new \
    -o ConnectTimeout=10 \
    -i "$SSH_KEY_PATH" \
    "$EC2_USER@$EC2_IP"

exit_code=$?
if [ $exit_code -ne 0 ]; then
    echo "Erreur lors de la connexion (code: $exit_code)"
    exit $exit_code
fi

================
File: -n ingress-nginx
================
{
    "SecurityGroups": [
        {
            "Description": "Security group for WordPress",
            "GroupName": "WP-SecurityGroup",
            "IpPermissions": [
                {
                    "FromPort": 80,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 80,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 30000,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 32767,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 31097,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 31097,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 22,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 22,
                    "UserIdGroupPairs": []
                },
                {
                    "FromPort": 443,
                    "IpProtocol": "tcp",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "ToPort": 443,
                    "UserIdGroupPairs": []
                }
            ],
            "OwnerId": "730335289383",
            "GroupId": "sg-0e2ab25ac27f8bbc5",
            "IpPermissionsEgress": [
                {
                    "IpProtocol": "-1",
                    "IpRanges": [
                        {
                            "CidrIp": "0.0.0.0/0"
                        }
                    ],
                    "Ipv6Ranges": [],
                    "PrefixListIds": [],
                    "UserIdGroupPairs": []
                }
            ],
            "Tags": [
                {
                    "Key": "book",
                    "Value": "book"
                }
            ],
            "VpcId": "vpc-0385cddb5bd815883"
        }
    ]
}

================
File: .gitignore
================
# Local .terraform directories
**/.terraform/*

# .tfstate files
*.tfstate
*.tfstate.*

# Crash log files
crash.log
crash.*.log

# Exclude all .tfvars files, which are likely to contain sensitive data
*.tfvars
*.tfvars.json

# Ignore override files as they are usually used to override resources locally
override.tf
override.tf.json
*_override.tf
*_override.tf.json

# Ignore CLI configuration files
.terraformrc
terraform.rc

# Ignore any .env files
*.env

# Ignore local development files
.vscode/
.idea/
