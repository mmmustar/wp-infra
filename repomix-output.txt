This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-01-30T12:52:23.163Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
ansible/
  group_vars/
    all.yml
  inventory/
    hosts.yml
  roles/
    common/
      defaults/
        main.yml
      files/
        get_secret.py
      handlers/
        main.yml
      tasks/
        main.yml
    docker/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        main.yml
    k3s/
      defaults/
        main.yml
      handlers/
        main.yml
      tasks/
        ebs-csi.yml
        helm.yml
        main.yml
      templates/
        ebs-csi-values.yml.j2
    nginx/
      handlers/
        main.yml
      tasks/
        main.yml
        ssl.yml
      templates/
        nginx-ssl-certificate.yaml.j2
    wordpress/
      defaults/
        main.yml
      tasks/
        main.yml
        tls.yml
      templates/
        configmap.yaml.j2
        deployment.yaml.j2
        ingress.yaml.j2
        secret.yaml.j2
        service.yaml.j2
        storage.yaml.j2
  ansible.cfg
  run.sh
  site.yml
environments/
  backend-config/
    .terraform.lock.hcl
    main.tf
    outputs.tf
    variables.tf
    versions.tf
  modules/
    compute/
      main.tf
      outputs.tf
      variables.tf
    k3s/
      iam.tf
    network/
      main.tf
      outputs.tf
      security.tf
      variables.tf
    security/
      main.tf
      outputs.tf
      variables.tf
  prod/
    backend.tf
  test/
    .terraform.lock.hcl
    backend.tf
    main.tf
    variables.tf
scripts/
  ssh-update.sh
.gitignore

================================================================
Files
================================================================

================
File: ansible/group_vars/all.yml
================
# ansible/group_vars/all.yml
---
# Variables globales communes à tous les serveurs
timezone: UTC
environment: test
aws_region: eu-west-3
ebs_csi_enabled: true
cloud_provider: aws
k8s_storage_class: ebs-sc

================
File: ansible/inventory/hosts.yml
================
all:
  vars:
    ansible_python_interpreter: /usr/bin/python3
    pip_package: python3-pip
    pip_install_packages:
      - name: boto3
  children:
    wordpress:
      hosts:
        wp-test:
          ansible_host: 35.180.222.29
          ansible_user: ubuntu
          ansible_ssh_private_key_file: ~/.ssh/test-aws-key-pair-new.pem

================
File: ansible/roles/common/defaults/main.yml
================
# roles/common/defaults/main.yml
---
system_packages:
  - apt-transport-https
  - ca-certificates
  - curl
  - software-properties-common
  - python3-pip
  - python3-boto3
  - python3-botocore
  - nfs-common
  - git
  - vim
timezone: UTC
environment: test

================
File: ansible/roles/common/files/get_secret.py
================
#!/usr/bin/env python3
import boto3
import json
import sys

try:
    client = boto3.client('secretsmanager', region_name='eu-west-3')
    response = client.get_secret_value(SecretId='book')
    print(response['SecretString'])
except Exception as e:
    print(f"Erreur: {str(e)}", file=sys.stderr)
    sys.exit(1)

================
File: ansible/roles/common/handlers/main.yml
================
# roles/common/handlers/main.yml
---
- name: restart systemd-resolved
  service:
    name: systemd-resolved
    state: restarted

================
File: ansible/roles/common/tasks/main.yml
================
---
- name: Wait for cloud-init to complete
  command: cloud-init status --wait
  register: cloud_init_result
  until: cloud_init_result.rc == 0
  retries: 30
  delay: 10
  changed_when: false
  
- name: Reset connection to allow group changes to take effect
  meta: reset_connection

- name: Wait for apt lock
  shell: while sudo fuser /var/lib/dpkg/lock >/dev/null 2>&1 || sudo fuser /var/lib/apt/lists/lock >/dev/null 2>&1; do sleep 1; done
  changed_when: false
  
- name: Clean apt
  apt:
    clean: yes
    autoclean: yes
    autoremove: yes
    force_apt_get: yes
  become: true

- name: Force kill apt/dpkg processes
  shell: |
    killall apt apt-get dpkg 2>/dev/null || true
    rm -f /var/lib/apt/lists/lock /var/cache/apt/archives/lock /var/lib/dpkg/lock* 2>/dev/null || true
    dpkg --configure -a
  changed_when: false
  ignore_errors: true
  become: true

- name: Update package cache
  apt:
    update_cache: yes
    cache_valid_time: 3600
    force_apt_get: yes
  register: apt_update
  retries: 5
  delay: 10
  until: apt_update is success
  become: true

- name: Install system packages
  apt:
    name:
      - python3
      - python3-pip
      - python3-boto3
      - python3-botocore
      - python3-kubernetes
      - python3-openshift
      - git
      - vim
    state: present
    force_apt_get: yes
    update_cache: yes
  become: true
  register: pkg_install
  retries: 3
  delay: 10
  until: pkg_install is success

================
File: ansible/roles/docker/defaults/main.yml
================
# roles/docker/defaults/main.yml
---
docker_users:
  - "{{ ansible_user }}"

================
File: ansible/roles/docker/handlers/main.yml
================
# roles/docker/handlers/main.yml
---
- name: restart docker
  service:
    name: docker
    state: restarted

================
File: ansible/roles/docker/tasks/main.yml
================
- name: Install required packages
  apt:
    name:
      - apt-transport-https
      - ca-certificates
      - curl
      - gnupg-agent
      - software-properties-common
    state: present

- name: Add Docker GPG key
  apt_key:
    url: https://download.docker.com/linux/ubuntu/gpg
    state: present

- name: Add Docker repository
  apt_repository:
    repo: deb [arch=amd64] https://download.docker.com/linux/ubuntu {{ ansible_distribution_release }} stable
    state: present

- name: Install Docker
  apt:
    name:
      - docker-ce
      - docker-ce-cli
      - containerd.io
      - docker-compose-plugin
    state: present
    update_cache: yes

- name: Start and enable Docker
  service:
    name: docker
    state: started
    enabled: yes

- name: Add users to docker group
  user:
    name: "{{ item }}"
    groups: docker
    append: yes
  with_items: "{{ docker_users }}"

- name: Configure Docker daemon
  copy:
    content: |
      {
        "log-driver": "json-file",
        "log-opts": {
          "max-size": "100m",
          "max-file": "3"
        }
      }
    dest: /etc/docker/daemon.json
  notify: restart docker

================
File: ansible/roles/k3s/defaults/main.yml
================
---
k3s_version: "v1.28.4+k3s2"
k3s_server_args: "--disable traefik --disable servicelb"
ebs_csi_enabled: true
ebs_csi_version: "2.26.0"
storage_class_name: "ebs-sc"
aws_region: "eu-west-3"
wp_namespace: wordpress
kubeconfig: /etc/rancher/k3s/k3s.yaml
security_group_id: "sg-0e2ab25ac27f8bbc5"

================
File: ansible/roles/k3s/handlers/main.yml
================
---
- name: restart k3s
  service:
    name: k3s
    state: restarted

- name: wait for k3s
  wait_for:
    path: "{{ kubeconfig }}"
    delay: 10
    timeout: 300

================
File: ansible/roles/k3s/tasks/ebs-csi.yml
================
---
- name: Fetch AWS Secrets
  command: "aws secretsmanager get-secret-value --secret-id book --region eu-west-3 --output json"
  register: secrets_result
  delegate_to: localhost
  become: false
  changed_when: false

- name: Set variables from secrets
  set_fact:
    aws_account_id: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_account_id') }}"
    aws_ebs_csi_role_name: "{{ (secrets_result.stdout | from_json).SecretString | from_json | json_query('aws_ebs_csi_role_name') }}"

- name: Check if EBS CSI Driver is already installed
  command: helm list -n kube-system
  register: helm_list
  changed_when: false

- name: Delete old StorageClass if exists
  command: kubectl delete storageclass ebs-sc
  ignore_errors: true
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Add Helm repo for EBS CSI Driver
  command: helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
  become: false
  when: ebs_csi_enabled
  ignore_errors: true

- name: Update Helm repos
  command: helm repo update
  become: false
  when: ebs_csi_enabled
  changed_when: true

- name: Template EBS CSI values
  template:
    src: ebs-csi-values.yml.j2
    dest: /tmp/ebs-csi-values.yml
    mode: '0644'
  when: ebs_csi_enabled

- name: Install EBS CSI Driver
  command: >
    helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver 
    --namespace kube-system 
    --version {{ ebs_csi_version }}
    -f /tmp/ebs-csi-values.yml
  when: ebs_csi_enabled and "aws-ebs-csi-driver" not in helm_list.stdout

- name: Upgrade EBS CSI Driver if already installed
  command: >
    helm upgrade aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver 
    --namespace kube-system 
    --version {{ ebs_csi_version }}
    -f /tmp/ebs-csi-values.yml
  when: ebs_csi_enabled and "aws-ebs-csi-driver" in helm_list.stdout

- name: Clean up temporary values file
  file:
    path: /tmp/ebs-csi-values.yml
    state: absent
  when: ebs_csi_enabled

================
File: ansible/roles/k3s/tasks/helm.yml
================
# roles/k3s/tasks/helm.yml
---
- name: Add Helm apt key
  apt_key:
    url: https://baltocdn.com/helm/signing.asc
    state: present

- name: Add Helm repository
  apt_repository:
    repo: deb https://baltocdn.com/helm/stable/debian/ all main
    state: present
    filename: helm

- name: Install Helm
  apt:
    name: helm
    state: present
    update_cache: yes

- name: Verify Helm installation
  command: helm version
  register: helm_version
  changed_when: false

================
File: ansible/roles/k3s/tasks/main.yml
================
---
- name: Run Terraform to get Public Subnet IDs
  shell: terraform -chdir=environments/test output -json public_subnet_ids
  register: terraform_output
  delegate_to: localhost
  changed_when: false
  become: false

- name: Parse the first subnet from Terraform output
  set_fact:
    public_subnet_id: "{{ (terraform_output.stdout | from_json)[0] }}"
  when: terraform_output.stdout != ""

- name: Download k3s
  get_url:
    url: https://get.k3s.io
    dest: /tmp/k3s-install.sh
    mode: '0755'

- name: Install k3s
  shell: INSTALL_K3S_VERSION={{ k3s_version }} /tmp/k3s-install.sh --write-kubeconfig-mode 644 {{ k3s_server_args }}
  args:
    creates: /usr/local/bin/k3s
  environment:
    INSTALL_K3S_EXEC: "{{ k3s_server_args }}"

- name: Wait for k3s to be ready
  wait_for:
    path: "{{ kubeconfig }}"
    delay: 10
    timeout: 300

- name: Wait for k3s node to be ready
  kubernetes.core.k8s_info:
    kind: Node
    name: "{{ ansible_hostname }}"
    kubeconfig: "{{ kubeconfig }}"
  register: node_status
  until: node_status.resources is defined and 
         node_status.resources | length > 0 and 
         node_status.resources[0].status.conditions | selectattr('type', 'equalto', 'Ready') | selectattr('status', 'equalto', 'True') | list | length > 0
  retries: 30
  delay: 10

- name: Wait for k3s node to be ready
  kubernetes.core.k8s_info:
    kind: Node
    name: "{{ ansible_hostname }}"
    kubeconfig: "{{ kubeconfig }}"
  register: node_status
  until: node_status.resources is defined and 
         node_status.resources | length > 0 and 
         node_status.resources[0].status.conditions | selectattr('type', 'equalto', 'Ready') | selectattr('status', 'equalto', 'True') | list | length > 0
  retries: 30
  delay: 10

- name: Add NodePort range and health check to security group
  shell: |
    aws ec2 authorize-security-group-ingress \
      --region {{ aws_region }} \
      --group-id {{ security_group_id }} \
      --ip-permissions '[
        {
          "IpProtocol": "tcp",
          "FromPort": 31097,
          "ToPort": 31097,
          "IpRanges": [{"CidrIp": "0.0.0.0/0"}]
        }
      ]'
  delegate_to: localhost
  become: false
  register: sg_result
  failed_when: 
    - sg_result.rc != 0 
    - '"InvalidPermission.Duplicate" not in sg_result.stderr'
  changed_when: sg_result.rc == 0

- name: Add NodePort range to security group
  shell: >
    aws ec2 authorize-security-group-ingress 
    --region {{ aws_region }}
    --group-id {{ security_group_id }}
    --protocol tcp 
    --port 30000-32767 
    --cidr 0.0.0.0/0
  delegate_to: localhost
  become: false
  register: sg_result
  failed_when: 
    - sg_result.rc != 0 
    - '"InvalidPermission.Duplicate" not in sg_result.stderr'
  changed_when: sg_result.rc == 0

- name: Install NGINX Ingress Controller
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    src: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml

- name: Wait for Ingress Controller deployment
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Deployment
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: ingress_deployment
  until: ingress_deployment.resources[0].status.availableReplicas is defined and ingress_deployment.resources[0].status.availableReplicas > 0
  retries: 30
  delay: 10

- name: Get Public Subnet ID for LoadBalancer
  command: >
    aws ec2 describe-subnets --filters "Name=tag:Name,Values=your-subnet-name"
    --query "Subnets[0].SubnetId" --output text --region eu-west-3
  register: public_subnet_result
  delegate_to: localhost
  changed_when: false

- name: Set public subnet ID as fact
  set_fact:
    public_subnet_id: "{{ public_subnet_result.stdout }}"
  when: public_subnet_result.stdout != ""

- name: Configure Ingress LoadBalancer
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition:
      apiVersion: v1
      kind: Service
      metadata:
        name: ingress-nginx-controller
        namespace: ingress-nginx
        annotations:
          service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
          service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: "true"
          service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
          service.beta.kubernetes.io/aws-load-balancer-subnets: "{{ public_subnet_id }}"
          service.beta.kubernetes.io/aws-load-balancer-manage-backend-security-group-rules: "true"
      spec:
        type: LoadBalancer
        externalTrafficPolicy: Local
        ports:
        - name: http
          port: 80
          protocol: TCP
          targetPort: 80
        - name: https
          port: 443
          protocol: TCP
          targetPort: 443
        selector:
          app.kubernetes.io/component: controller
          app.kubernetes.io/instance: ingress-nginx
          app.kubernetes.io/name: ingress-nginx

- name: Wait for LoadBalancer external IP
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Service
    name: ingress-nginx-controller
    namespace: ingress-nginx
  register: lb_service
  until: lb_service.resources[0].status.loadBalancer.ingress is defined
  retries: 30
  delay: 10

================
File: ansible/roles/k3s/templates/ebs-csi-values.yml.j2
================
controller:
  region: "{{ aws_region }}"
  serviceAccount:
    create: true
    name: ebs-csi-controller-sa
    annotations:
      # Pour EC2 et non EKS
      eks.amazonaws.com/role-arn: null
      
node:
  serviceAccount:
    create: true
    name: ebs-csi-node-sa

storageClasses:
- name: ebs-sc
  provisioner: ebs.csi.aws.com  # Changement ici
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
  volumeBindingMode: WaitForFirstConsumer  # Changement ici
  allowVolumeExpansion: true
  reclaimPolicy: Delete
  parameters:
    type: gp3

================
File: ansible/roles/nginx/handlers/main.yml
================
- name: Reload Nginx
  ansible.builtin.service:
    name: nginx
    state: reloaded
  become: true

- name: Restart Nginx
  ansible.builtin.service:
    name: nginx
    state: restarted
  become: true

- name: Enable Nginx
  ansible.builtin.service:
    name: nginx
    enabled: true
  become: true

- name: Test Nginx configuration
  ansible.builtin.command:
    cmd: nginx -t
  become: true
  register: nginx_config_test
  changed_when: false

================
File: ansible/roles/nginx/tasks/main.yml
================
---
- name: Force kill apt/dpkg processes
  shell: |
    killall apt apt-get dpkg 2>/dev/null || true
    rm -f /var/lib/apt/lists/lock /var/cache/apt/archives/lock /var/lib/dpkg/lock* 2>/dev/null || true
    dpkg --configure -a
  changed_when: false
  ignore_errors: true
  become: true

- name: Update package cache
  apt:
    update_cache: yes
    cache_valid_time: 3600
  register: apt_update
  retries: 3
  delay: 5
  until: apt_update is success
  become: true

- name: Install Nginx
  apt:
    name: nginx
    state: present
  become: true
  register: nginx_install
  retries: 3
  delay: 5
  until: nginx_install is success

- name: Copy nginx SSL configuration
  template:
    src: nginx-ssl-certificate.yaml.j2
    dest: /etc/nginx/sites-available/default
  notify: Reload Nginx
  become: true

- name: Enable nginx site
  file:
    src: /etc/nginx/sites-available/default
    dest: /etc/nginx/sites-enabled/default
    state: link
  notify: Reload Nginx
  become: true

================
File: ansible/roles/nginx/tasks/ssl.yml
================
# ansible/roles/nginx/tasks/ssl.yml
---
- name: Create SSL directory
  file:
    path: /etc/nginx/ssl
    state: directory
    mode: '0755'
  become: true

- name: Generate SSL private key
  command: openssl genpkey -algorithm RSA -out /etc/nginx/ssl/nginx-selfsigned.key -pkeyopt rsa_keygen_bits:2048
  args:
    creates: /etc/nginx/ssl/nginx-selfsigned.key
  become: true

- name: Generate SSL certificate
  command: openssl req -new -x509 -key /etc/nginx/ssl/nginx-selfsigned.key -out /etc/nginx/ssl/nginx-selfsigned.crt -days 365 -subj "/CN={{ nginx_server_name }}"
  args:
    creates: /etc/nginx/ssl/nginx-selfsigned.crt
  become: true

================
File: ansible/roles/nginx/templates/nginx-ssl-certificate.yaml.j2
================
# ansible/roles/k3s/templates/nginx-ssl-certificate.j2
server {
    listen 80;
    server_name {{ nginx_server_name }};
    return 301 https://$host$request_uri;
}

server {
    listen 443 ssl;
    server_name {{ nginx_server_name }};

    ssl_certificate /etc/ssl/certs/nginx-selfsigned.crt;
    ssl_certificate_key /etc/ssl/private/nginx-selfsigned.key;

    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_prefer_server_ciphers on;
    ssl_ciphers "ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-CHACHA20-POLY1305:ECDHE-RSA-CHACHA20-POLY1305:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA256";

    location / {
        proxy_pass http://localhost:3000; # Remplacez par le port de votre application
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

================
File: ansible/roles/wordpress/defaults/main.yml
================
# roles/wordpress/defaults/main.yml
---
wp_namespace: wordpress
wp_domain: test.mmustar.fr
wp_replicas: 1
storage_class_name: local-path
storage_size: 10Gi
wp_memory_request: 256Mi
wp_memory_limit: 512Mi
wp_cpu_request: 250m
wp_cpu_limit: 500m
kubeconfig: /etc/rancher/k3s/k3s.yaml

================
File: ansible/roles/wordpress/tasks/main.yml
================
# roles/wordpress/tasks/main.yml
---
- name: Update apt cache
  apt:
    update_cache: yes
  become: true

- name: Install Python base packages
  apt:
    name: 
      - python3-yaml
      - python3-jsonpatch
      - python3-kubernetes
    state: present
  become: true

- name: Install NGINX Ingress Controller
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    src: https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/cloud/deploy.yaml
  register: nginx_install

- name: Wait for Ingress Controller to be ready
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Pod
    namespace: ingress-nginx
    label_selectors:
      - app.kubernetes.io/component=controller
  register: ingress_pods
  until: ingress_pods.resources | length > 0 and ingress_pods.resources[0].status.phase == "Running"
  retries: 30
  delay: 20
  ignore_errors: true

- name: Check Ingress Controller logs if deployment failed
  command: kubectl logs -n ingress-nginx -l app.kubernetes.io/component=controller --tail=50
  register: ingress_logs
  when: ingress_pods.failed is defined
  ignore_errors: true

- name: Create WordPress namespace
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    definition:
      apiVersion: v1
      kind: Namespace
      metadata:
        name: "{{ wp_namespace }}"

- name: Include TLS configuration tasks
  include_tasks: tls.yml
  tags: 
    - wordpress
    - tls

- name: Create WordPress resources
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    template: "{{ item }}"
    namespace: "{{ wp_namespace }}"
  loop:
    - storage.yaml.j2
    - configmap.yaml.j2
    - secret.yaml.j2
    - deployment.yaml.j2
    - service.yaml.j2

- name: Apply Ingress
  kubernetes.core.k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    template: ingress.yaml.j2
    namespace: "{{ wp_namespace }}"

- name: Wait for WordPress deployment
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Deployment
    name: wordpress
    namespace: "{{ wp_namespace }}"
  register: wp_deployment
  until: wp_deployment.resources[0].status.availableReplicas is defined and wp_deployment.resources[0].status.availableReplicas > 0
  retries: 30
  delay: 10

- name: Verify Ingress status
  kubernetes.core.k8s_info:
    kubeconfig: "{{ kubeconfig }}"
    kind: Ingress
    name: wordpress-ingress
    namespace: "{{ wp_namespace }}"
  register: ingress_status
  until: ingress_status.resources[0].status.loadBalancer is defined
  retries: 30
  delay: 10

================
File: ansible/roles/wordpress/tasks/tls.yml
================
---
- name: Create temporary directory
  file:
    path: "/tmp/ssl-{{ wp_domain }}"
    state: directory
    mode: '0755'
  delegate_to: localhost
  become: false

- name: Create OpenSSL private key
  openssl_privatekey:
    path: "/tmp/ssl-{{ wp_domain }}/tls.key"
    size: 2048
  delegate_to: localhost
  become: false

- name: Create OpenSSL CSR
  openssl_csr:
    path: "/tmp/ssl-{{ wp_domain }}/tls.csr"
    privatekey_path: "/tmp/ssl-{{ wp_domain }}/tls.key"
    common_name: "{{ wp_domain }}"
  delegate_to: localhost
  become: false

- name: Create self-signed certificate
  openssl_certificate:
    path: "/tmp/ssl-{{ wp_domain }}/tls.crt"
    privatekey_path: "/tmp/ssl-{{ wp_domain }}/tls.key"
    csr_path: "/tmp/ssl-{{ wp_domain }}/tls.csr"
    provider: selfsigned
  delegate_to: localhost
  become: false

- name: Create Kubernetes TLS secret
  k8s:
    kubeconfig: "{{ kubeconfig }}"
    state: present
    namespace: "{{ wp_namespace }}"
    definition:
      apiVersion: v1
      kind: Secret
      metadata:
        name: wordpress-tls
      type: kubernetes.io/tls
      data:
        tls.crt: "{{ lookup('file', '/tmp/ssl-' + wp_domain + '/tls.crt') | b64encode }}"
        tls.key: "{{ lookup('file', '/tmp/ssl-' + wp_domain + '/tls.key') | b64encode }}"

- name: Clean up temporary certificate files
  file:
    path: "/tmp/ssl-{{ wp_domain }}"
    state: absent
  delegate_to: localhost
  become: false

================
File: ansible/roles/wordpress/templates/configmap.yaml.j2
================
apiVersion: v1
kind: ConfigMap
metadata:
  name: wordpress-config
data:
  WORDPRESS_DB_HOST: "{{ db_host }}"
  WORDPRESS_DB_NAME: "{{ db_name }}"
  WORDPRESS_DB_USER: "{{ db_user }}"

================
File: ansible/roles/wordpress/templates/deployment.yaml.j2
================
# roles/wordpress/templates/deployment.yaml.j2
apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
spec:
  replicas: {{ wp_replicas }}
  selector:
    matchLabels:
      app: wordpress
  template:
    metadata:
      labels:
        app: wordpress
    spec:
      containers:
      - name: wordpress
        image: wordpress:latest
        ports:
        - containerPort: 80
        envFrom:
        - configMapRef:
            name: wordpress-config
        - secretRef:
            name: wordpress-secret
        volumeMounts:
        - name: wordpress-data
          mountPath: /var/www/html
        resources:
          requests:
            memory: "{{ wp_memory_request }}"
            cpu: "{{ wp_cpu_request }}"
          limits:
            memory: "{{ wp_memory_limit }}"
            cpu: "{{ wp_cpu_limit }}"
      volumes:
      - name: wordpress-data
        persistentVolumeClaim:
          claimName: wp-pvc

================
File: ansible/roles/wordpress/templates/ingress.yaml.j2
================
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: wordpress-ingress
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/proxy-body-size: "50m"
    nginx.ingress.kubernetes.io/ssl-protocols: "TLSv1.2 TLSv1.3"
    nginx.ingress.kubernetes.io/ssl-prefer-server-ciphers: "on"
    nginx.ingress.kubernetes.io/ssl-ciphers: "ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-RSA-AES256-GCM-SHA384"
spec:
  tls:
  - hosts:
    - {{ wp_domain }}
    secretName: wordpress-tls
  rules:
  - host: {{ wp_domain }}
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: wordpress
            port:
              number: 80

================
File: ansible/roles/wordpress/templates/secret.yaml.j2
================
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-secret
type: Opaque
data:
  WORDPRESS_DB_PASSWORD: "{{ db_secrets.MYSQL_PASSWORD | b64encode }}"
  WORDPRESS_DB_HOST: "{{ db_secrets.MYSQL_HOST | b64encode }}"
  WORDPRESS_DB_USER: "{{ db_secrets.MYSQL_USER | b64encode }}"
  WORDPRESS_DB_NAME: "{{ db_secrets.MYSQL_DATABASE | b64encode }}"

================
File: ansible/roles/wordpress/templates/service.yaml.j2
================
apiVersion: v1
kind: Service
metadata:
  name: wordpress
spec:
  selector:
    app: wordpress
  ports:
  - port: 80
    targetPort: 80
  type: ClusterIP

================
File: ansible/roles/wordpress/templates/storage.yaml.j2
================
# roles/wordpress/templates/storage.yaml.j2
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "{{ storage_size }}"
  storageClassName: "{{ storage_class_name }}"

================
File: ansible/ansible.cfg
================
[defaults]
inventory = inventory/hosts.yml
private_key_file = ~/.ssh/test-aws-key-pair-new.pem
host_key_checking = False
remote_user = ubuntu
retry_files_enabled = False
gathering = smart
fact_caching = jsonfile
fact_caching_connection = ~/.ansible/facts_cache
log_path = ~/.ansible/ansible.log
roles_path = roles
nocows = True

[ssh_connection]
pipelining = True
ssh_args = -o ControlMaster=auto -o ControlPersist=600s -o ServerAliveInterval=30 -o ServerAliveCountMax=10 -o TCPKeepAlive=yes
control_path = %(directory)s/%%h-%%r
control_path_dir = ~/.ansible/cp
retries = 5
timeout = 60

================
File: ansible/run.sh
================
# run.sh
#!/bin/bash
set -euo pipefail

# Vérification des variables d'environnement requises
if [[ -z "${AWS_ACCESS_KEY_ID:-}" ]] || [[ -z "${AWS_SECRET_ACCESS_KEY:-}" ]]; then
    echo "Erreur: Les credentials AWS doivent être définis"
    exit 1
fi

# Vérification de la présence des fichiers nécessaires
required_files=("site.yml" "ansible.cfg" "inventory/hosts.yml")
for file in "${required_files[@]}"; do
    if [[ ! -f "$file" ]]; then
        echo "Erreur: Fichier requis manquant: $file"
        exit 1
    fi
done

# Vérification de la syntaxe
echo "Vérification de la syntaxe du playbook..."
if ! ansible-playbook --syntax-check site.yml; then
    echo "Erreur: Le playbook contient des erreurs de syntaxe"
    exit 1
fi

# Exécution du playbook
echo "Exécution du playbook..."
ansible-playbook site.yml -v

# Vérification du statut de sortie
if [[ $? -eq 0 ]]; then
    echo "Le playbook s'est exécuté avec succès"
else
    echo "Erreur lors de l'exécution du playbook"
    exit 1
fi

================
File: ansible/site.yml
================
# ansible/site.yml
---
- name: Configuration complète des serveurs WordPress
  hosts: wordpress
  become: true
  vars:
    aws_region: eu-west-3
    ebs_csi_enabled: true
    nginx_server_name: test.mmustar.fr
  pre_tasks:
    - name: Get AWS secret
      block:
        - script: roles/common/files/get_secret.py
          register: secret_output
          delegate_to: localhost
          become: false
          changed_when: false
      rescue:
        - fail:
            msg: "Impossible de récupérer les secrets AWS. Vérifiez vos credentials AWS et l'existence du secret."

    - name: Set database secrets
      set_fact:
        db_secrets: "{{ secret_output.stdout | from_json }}"
      no_log: true

    - name: Set database variables  
      set_fact:
        db_host: "{{ db_secrets.MYSQL_HOST }}"
        db_name: "{{ db_secrets.MYSQL_DATABASE }}"
        db_user: "{{ db_secrets.MYSQL_USER }}"
        db_password: "{{ db_secrets.MYSQL_PASSWORD }}"
      no_log: true

  roles:
    - { role: common, tags: ['common'] }
    - { role: docker, tags: ['docker'] }
    - { role: k3s, tags: ['k3s'] }
    - { role: wordpress, tags: ['wordpress'] }
    - { role: nginx, tags: ['nginx'] }

================
File: environments/backend-config/.terraform.lock.hcl
================
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.0.0"
  constraints = "~> 5.0"
  hashes = [
    "h1:swP2uqDPi7bRLe+J4oUGEp8ZPTG4NaAV9QK+Iqgo2ro=",
  ]
}

================
File: environments/backend-config/main.tf
================
# environments/backend-config/main.tf

# Un seul bucket S3 pour tous les environnements
resource "aws_s3_bucket" "terraform_state" {
  bucket = "${var.project_name}-terraform-state"

  lifecycle {
    prevent_destroy = true
  }

  tags = {
    Name    = "${var.project_name}-terraform-state"
    Project = var.project_name
  }
}

# Activation du versioning
resource "aws_s3_bucket_versioning" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id
  versioning_configuration {
    status = "Enabled"
  }
}

# Activation du chiffrement
resource "aws_s3_bucket_server_side_encryption_configuration" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  rule {
    apply_server_side_encryption_by_default {
      sse_algorithm = "AES256"
    }
  }
}

# Blocage de l'accès public
resource "aws_s3_bucket_public_access_block" "terraform_state" {
  bucket = aws_s3_bucket.terraform_state.id

  block_public_acls       = true
  block_public_policy     = true
  ignore_public_acls      = true
  restrict_public_buckets = true
}

# Une seule table DynamoDB pour le verrouillage
resource "aws_dynamodb_table" "terraform_locks" {
  name         = "${var.project_name}-terraform-locks"
  billing_mode = "PAY_PER_REQUEST"
  hash_key     = "LockID"

  attribute {
    name = "LockID"
    type = "S"
  }

  tags = {
    Name    = "${var.project_name}-terraform-locks"
    Project = var.project_name
  }
}

================
File: environments/backend-config/outputs.tf
================
# environments/backend-config/outputs.tf
output "bucket_name" {
  description = "Nom du bucket S3 pour le state Terraform"
  value       = aws_s3_bucket.terraform_state.id
}

output "dynamodb_table_name" {
  description = "Nom de la table DynamoDB pour le verrouillage"
  value       = aws_dynamodb_table.terraform_locks.name
}

================
File: environments/backend-config/variables.tf
================
# environments/backend-config/variables.tf
variable "aws_region" {
  description = "AWS region"
  type        = string
  default     = "eu-west-3"
}

variable "project_name" {
  description = "Nom du projet"
  type        = string
  default     = "wordpress-mmustar"
}

================
File: environments/backend-config/versions.tf
================
# environments/backend-config/versions.tf
terraform {
  required_version = ">= 1.0.0"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
}

provider "aws" {
  region = var.aws_region
  
  default_tags {
    tags = {
      Project     = var.project_name
      ManagedBy   = "terraform"
    }
  }
}

================
File: environments/modules/compute/main.tf
================
# environments/modules/compute/main.tf
data "aws_ami" "ubuntu" {
  most_recent = true
  owners      = ["099720109477"] # Canonical

  filter {
    name   = "name"
    values = ["ubuntu/images/hvm-ssd/ubuntu-focal-20.04-amd64-server-*"]
  }
}

resource "aws_instance" "wordpress" {
  ami           = data.aws_ami.ubuntu.id
  instance_type = var.instance_type
  subnet_id     = var.subnet_id
  key_name      = var.key_name

  root_block_device {
    volume_size = 20
    volume_type = "gp3"
  }

  tags = {
    Name        = "${var.project_name}-instance-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    prevent_destroy = true
  }
}

resource "aws_eip" "wordpress" {
  count    = var.environment == "test" ? 1 : 0
  instance = aws_instance.wordpress.id
  domain   = "vpc"

  tags = {
    Name        = "${var.project_name}-eip-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/compute/outputs.tf
================
# environments/modules/compute/outputs.tf
output "instance_id" {
  description = "ID of the created EC2 instance"
  value       = aws_instance.wordpress.id
}

output "instance_public_ip" {
  description = "Public IP of the EC2 instance"
  value       = var.environment == "test" ? aws_eip.wordpress[0].public_ip : aws_instance.wordpress.public_ip
}

output "instance_private_ip" {
  description = "Private IP of the EC2 instance"
  value       = aws_instance.wordpress.private_ip
}

================
File: environments/modules/compute/variables.tf
================
# environments/modules/compute/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the instances will be created"
  type        = string
}

variable "subnet_id" {
  description = "Subnet ID where the instances will be created"
  type        = string
}

variable "instance_type" {
  description = "EC2 instance type"
  type        = string
  default     = "t2.micro"
}

variable "key_name" {
  description = "Name of the SSH key pair"
  type        = string
}

================
File: environments/modules/k3s/iam.tf
================
# modules/k3s/iam.tf
resource "aws_iam_openid_connect_provider" "k3s" {
  url             = "https://token.actions.githubusercontent.com"
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = ["1b511abead59c6ce207077c0bf0e0043b1382612"]
}

resource "aws_iam_role" "ebs_csi" {
  name = "${var.environment}-ebs-csi"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [{
      Effect = "Allow"
      Principal = {
        Federated = aws_iam_openid_connect_provider.k3s.arn
      }
      Action = "sts:AssumeRoleWithWebIdentity"
      Condition = {
        StringEquals = {
          "${aws_iam_openid_connect_provider.k3s.url}:sub": "system:serviceaccount:kube-system:ebs-csi-controller-sa"
        }
      }
    }]
  })
}

resource "aws_iam_role_policy" "ebs_csi" {
  name = "${var.environment}-ebs-csi-policy"
  role = aws_iam_role.ebs_csi.id

  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Action = [
          "ec2:CreateSnapshot",
          "ec2:AttachVolume",
          "ec2:DetachVolume",
          "ec2:ModifyVolume",
          "ec2:DescribeAvailabilityZones",
          "ec2:DescribeInstances",
          "ec2:DescribeSnapshots",
          "ec2:DescribeTags",
          "ec2:DescribeVolumes",
          "ec2:DescribeVolumesModifications",
          "ec2:CreateVolume",
          "ec2:DeleteVolume",
          "ec2:CreateTags",
          "ec2:DeleteTags"
        ]
        Resource = "*"
      }
    ]
  })
}

variable "environment" {
  description = "Environment name"
  type        = string
}

output "ebs_csi_role_arn" {
  value = aws_iam_role.ebs_csi.arn
}

================
File: environments/modules/network/main.tf
================
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_hostnames = true
  enable_dns_support   = true

  tags = {
    Name        = "${var.project_name}-vpc-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id

  tags = {
    Name        = "${var.project_name}-igw-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_subnet" "public" {
  count                  = length(var.public_subnet_cidrs)
  vpc_id                 = aws_vpc.main.id
  cidr_block             = var.public_subnet_cidrs[count.index]
  availability_zone      = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true

  tags = {
    Name        = "${var.project_name}-public-subnet-${count.index + 1}-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id

  route {
    cidr_block = "0.0.0.0/0"
    gateway_id = aws_internet_gateway.main.id
  }

  tags = {
    Name        = "${var.project_name}-public-rt-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

resource "aws_route_table_association" "public" {
  count = length(var.public_subnet_cidrs)

  subnet_id      = aws_subnet.public[count.index].id
  route_table_id = aws_route_table.public.id
}

data "aws_availability_zones" "available" {
  state = "available"
}

================
File: environments/modules/network/outputs.tf
================
# environments/modules/network/outputs.tf
output "public_subnet_ids" {
  value = aws_subnet.public[*].id
}

output "vpc_id" {
  value = aws_vpc.main.id
}

output "route_table_id" {
  value = aws_route_table.public.id
}

================
File: environments/modules/network/security.tf
================
# environments/modules/network/security.tf
resource "aws_security_group" "wordpress" {
  name_prefix = "${var.project_name}-sg-${var.environment}"
  vpc_id      = aws_vpc.main.id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]  # À restreindre selon vos besoins
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "${var.project_name}-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/network/variables.tf
================
# environments/modules/network/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
  default     = "wordpress-mmustar"
}

# environments/modules/network/variables.tf
variable "vpc_cidr" {
  description = "CIDR block for VPC"
  type        = string
  default     = "172.16.0.0/16"  # Modifié pour éviter le conflit avec le RDS VPC
}

variable "public_subnet_cidrs" {
  description = "CIDR blocks for public subnets"
  type        = list(string)
  default     = ["172.16.1.0/24", "172.16.2.0/24"]  # Ajusté en conséquence
}

================
File: environments/modules/security/main.tf
================
# environments/modules/security/main.tf
resource "aws_security_group" "wordpress" {
  name_prefix = "${var.project_name}-wp-${var.environment}"
  description = "Security group for WordPress instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTP"
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow HTTPS"
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"] # À restreindre selon vos besoins
    description = "Allow SSH"
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
    description = "Allow all outbound traffic"
  }

  tags = {
    Name        = "${var.project_name}-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    create_before_destroy = true
  }
}

resource "aws_security_group" "rds" {
  name_prefix = "${var.project_name}-rds-${var.environment}"
  description = "Security group for RDS instance"
  vpc_id      = var.vpc_id

  ingress {
    from_port       = 3306
    to_port         = 3306
    protocol        = "tcp"
    security_groups = [aws_security_group.wordpress.id]
    description     = "Allow MySQL from WordPress SG"
  }

  tags = {
    Name        = "${var.project_name}-rds-sg-${var.environment}"
    Environment = var.environment
    Project     = var.project_name
  }
}

================
File: environments/modules/security/outputs.tf
================
# environments/modules/security/outputs.tf
output "wordpress_sg_id" {
  description = "ID of WordPress security group"
  value       = aws_security_group.wordpress.id
}

output "rds_sg_id" {
  description = "ID of RDS security group"
  value       = aws_security_group.rds.id
}

================
File: environments/modules/security/variables.tf
================
# environments/modules/security/variables.tf
variable "environment" {
  description = "Environment name (test/prod)"
  type        = string
}

variable "project_name" {
  description = "Project name for resource tagging"
  type        = string
}

variable "vpc_id" {
  description = "VPC ID where the security groups will be created"
  type        = string
}

================
File: environments/prod/backend.tf
================
# environments/prod/backend.tf
terraform {
  backend "s3" {
    bucket         = "wordpress-mmustar-terraform-state"
    key            = "environments/prod/terraform.tfstate"
    region         = "eu-west-3"
    dynamodb_table = "wordpress-mmustar-terraform-locks"
    encrypt        = true
  }
}

================
File: environments/test/.terraform.lock.hcl
================
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.0.0"
  constraints = "~> 5.0"
  hashes = [
    "h1:swP2uqDPi7bRLe+J4oUGEp8ZPTG4NaAV9QK+Iqgo2ro=",
  ]
}

================
File: environments/test/backend.tf
================
# environments/test/backend.tf
terraform {
  backend "s3" {
    bucket         = "wordpress-mmustar-terraform-state"
    key            = "test/terraform.tfstate"
    region         = "eu-west-3"
    dynamodb_table = "wordpress-mmustar-terraform-locks"
    encrypt        = true
  }
}

================
File: environments/test/main.tf
================
# Provider configuration
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  required_version = ">= 1.0.0"
}

provider "aws" {
  region = "eu-west-3"
}

# Variables
variable "project_name" {
  description = "The name of the project"
  type        = string
}

variable "environment" {
  description = "The environment for deployment"
  type        = string
}

# Modules
module "network" {
  source              = "../modules/network"
  environment         = var.environment
  project_name        = var.project_name
  vpc_cidr            = "10.0.0.0/16"
  public_subnet_cidrs = ["10.0.1.0/24", "10.0.2.0/24"]
}

module "k3s" {
  source      = "../modules/k3s"
  environment = var.environment
}

# Security Group
resource "aws_security_group" "wordpress_test" {
  name_prefix = "WP-SecurityGroup-Test-"
  description = "Security group for WordPress test instance"
  vpc_id      = module.network.vpc_id

  ingress {
    from_port   = 80
    to_port     = 80
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 443
    to_port     = 443
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 22
    to_port     = 22
    protocol    = "tcp"
    cidr_blocks = ["0.0.0.0/0"]
  }

  ingress {
    from_port   = 3306
    to_port     = 3306
    protocol    = "tcp"
    cidr_blocks = ["10.0.0.0/16"]
  }

  egress {
    from_port   = 0
    to_port     = 0
    protocol    = "-1"
    cidr_blocks = ["0.0.0.0/0"]
  }

  tags = {
    Name        = "WP-SG-Test"
    Environment = var.environment
    Project     = var.project_name
  }

  depends_on = [module.network]
}

# Existing EIP
data "aws_eip" "wordpress_test" {
  id = "eipalloc-0933b219497dd6c15"
}

# Data source pour le rôle IAM existant
data "aws_iam_role" "ec2_secrets_manager_role" {
  name = "EC2SecretsManagerRole"
}

# IAM Instance Profile pour EC2
resource "aws_iam_instance_profile" "ec2_secrets_manager_profile" {
  name = "ec2-secrets-manager-profile"
  role = data.aws_iam_role.ec2_secrets_manager_role.name
}

# EC2 Instance
resource "aws_instance" "wordpress_test" {
  ami                         = "ami-06e02ae7bdac6b938"
  instance_type               = "t2.micro"
  subnet_id                   = module.network.public_subnet_ids[0]
  vpc_security_group_ids      = [aws_security_group.wordpress_test.id]
  key_name                    = "test-aws-key-pair-new"
  associate_public_ip_address = true
  iam_instance_profile        = aws_iam_instance_profile.ec2_secrets_manager_profile.name

  root_block_device {
    volume_size           = 8
    volume_type          = "gp3"
    delete_on_termination = true
  }

  tags = {
    Name        = "WP-Instance-Test"
    Environment = var.environment
    Project     = var.project_name
  }

  lifecycle {
    create_before_destroy = true
  }

  depends_on = [
    module.network,
    aws_security_group.wordpress_test,
    aws_iam_instance_profile.ec2_secrets_manager_profile
  ]
}

# EIP Association
resource "aws_eip_association" "wordpress_test" {
  instance_id   = aws_instance.wordpress_test.id
  allocation_id = data.aws_eip.wordpress_test.id
  depends_on    = [aws_instance.wordpress_test]
}

# RDS Security Group and Instance data sources
data "aws_security_group" "rds" {
  id = "sg-00efe258e85b22a30"
}

data "aws_db_instance" "wordpress" {
  db_instance_identifier = "wordpress-db"
}

# Outputs
output "rds_endpoint" {
  value = data.aws_db_instance.wordpress.endpoint
}

output "instance_public_ip" {
  value = aws_instance.wordpress_test.public_ip
}

output "instance_id" {
  value = aws_instance.wordpress_test.id
}

output "eip_public_ip" {
  value = data.aws_eip.wordpress_test.public_ip
}

output "ebs_csi_role_arn" {
  value = module.k3s.ebs_csi_role_arn
}

================
File: environments/test/variables.tf
================
# environments/test/variables.tf
variable "aws_region" {
  description = "AWS Region"
  type        = string
  default     = "eu-west-3"
}

# SSH key pour instance EC2
variable "key_name" {
  description = "SSH key pair name"
  type        = string
  default     = "test-aws-key-pair-new"  # Remplacez par votre nom de clé
  
}

================
File: scripts/ssh-update.sh
================
#!/bin/bash

# Configuration par défaut
SSH_KEY_PATH="/home/gnou/.ssh/test-aws-key-pair-new.pem"
EC2_USER="ubuntu"
KNOWN_HOSTS="/home/gnou/.ssh/known_hosts"

# Fonction d'aide
show_usage() {
    echo "Usage: $0 [OPTIONS] EC2_IP_ADDRESS"
    echo "Options:"
    echo "  -k, --key PATH    Chemin vers la clé SSH (default: $SSH_KEY_PATH)"
    echo "  -u, --user USER   Utilisateur EC2 (default: $EC2_USER)"
    echo "  -h, --help        Affiche cette aide"
    exit 1
}

# Traitement des arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -k|--key)
            SSH_KEY_PATH="$2"
            shift 2
            ;;
        -u|--user)
            EC2_USER="$2"
            shift 2
            ;;
        -h|--help)
            show_usage
            ;;
        *)
            EC2_IP="$1"
            shift
            ;;
    esac
done

# Vérification des paramètres requis
if [ -z "$EC2_IP" ]; then
    echo "Erreur: Adresse IP de l'instance EC2 manquante"
    show_usage
fi

if [ ! -f "$SSH_KEY_PATH" ]; then
    echo "Erreur: Clé SSH non trouvée: $SSH_KEY_PATH"
    exit 1
fi

# Fonction pour vérifier si l'instance est accessible
check_instance() {
    timeout 5 nc -zv $EC2_IP 22 &>/dev/null
    return $?
}

# Attendre que l'instance soit accessible
echo "Vérification de l'accessibilité de l'instance..."
ATTEMPTS=0
MAX_ATTEMPTS=10

while ! check_instance; do
    ATTEMPTS=$((ATTEMPTS + 1))
    if [ $ATTEMPTS -ge $MAX_ATTEMPTS ]; then
        echo "Erreur: Impossible de se connecter à l'instance après $MAX_ATTEMPTS tentatives"
        exit 1
    fi
    echo "Instance non accessible, nouvelle tentative dans 5 secondes... ($ATTEMPTS/$MAX_ATTEMPTS)"
    sleep 5
done

# Supprimer l'ancienne clé host si elle existe
if ssh-keygen -F $EC2_IP >/dev/null 2>&1; then
    echo "Suppression de l'ancienne clé host..."
    ssh-keygen -f "$KNOWN_HOSTS" -R "$EC2_IP" >/dev/null 2>&1
fi

# Tentative de connexion
echo "Connexion à l'instance $EC2_IP..."
ssh -o StrictHostKeyChecking=accept-new \
    -o ConnectTimeout=10 \
    -i "$SSH_KEY_PATH" \
    "$EC2_USER@$EC2_IP"

exit_code=$?
if [ $exit_code -ne 0 ]; then
    echo "Erreur lors de la connexion (code: $exit_code)"
    exit $exit_code
fi

================
File: .gitignore
================
# Local .terraform directories
**/.terraform/*

# .tfstate files
*.tfstate
*.tfstate.*

# Crash log files
crash.log
crash.*.log

# Exclude all .tfvars files, which are likely to contain sensitive data
*.tfvars
*.tfvars.json

# Ignore override files as they are usually used to override resources locally
override.tf
override.tf.json
*_override.tf
*_override.tf.json

# Ignore CLI configuration files
.terraformrc
terraform.rc

# Ignore any .env files
*.env

# Ignore local development files
.vscode/
.idea/
